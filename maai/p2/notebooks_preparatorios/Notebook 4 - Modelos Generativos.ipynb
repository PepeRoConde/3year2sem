{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57knM8jrYZ2t"
   },
   "source": [
    "# Notebook 4: Modelos generativos\n",
    "\n",
    "## Pre-requisitos\n",
    "\n",
    "### Instalar paquetes\n",
    "\n",
    "Si la práctica requiere algún paquete de Python, habrá que incluir una celda en la que se instalen. Si usamos un paquete que se ha utilizado en prácticas anteriores, podríamos dar por supuesto que está instalado pero no cuesta nada satisfacer todas las dependencias en la propia práctica para reducir las dependencias entre ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkaimNJfYZ2w"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de instalación de tensorflow 2.0\n",
    "#%tensorflow_version 2.x\n",
    "# !pip3 install tensorflow  # NECESARIO SOLO SI SE EJECUTA EN LOCAL\n",
    "import tensorflow as tf\n",
    "\n",
    "# Hacemos los imports que sean necesarios\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOch-CnwQttl"
   },
   "source": [
    "# Modelos generativos sobre MNIST\n",
    "\n",
    "Lo primero que tenemos que hacer es cargar el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gXdWDBIEKel"
   },
   "outputs": [],
   "source": [
    "labeled_data = 0.01 # Vamos a usar el etiquetado de sólo el 1% de los datos\n",
    "np.random.seed(42)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "indexes = np.arange(len(x_train))\n",
    "np.random.shuffle(indexes)\n",
    "ntrain_data = int(labeled_data*len(x_train))\n",
    "unlabeled_train = x_train[indexes[ntrain_data:]]\n",
    "x_train = x_train[indexes[:ntrain_data]]\n",
    "y_train = y_train[indexes[:ntrain_data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XsZIqV8TmSc"
   },
   "outputs": [],
   "source": [
    "# TODO: Haz el preprocesado que necesites aquí (si lo necesitas)\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1]*x_train.shape[2]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1]*x_test.shape[2]))\n",
    "unlabeled_train = np.reshape(unlabeled_train, (unlabeled_train.shape[0], unlabeled_train.shape[1]*unlabeled_train.shape[2]))\n",
    "\n",
    "one_hot_train = np.zeros((y_train.size, len(set(y_train))), dtype=int)\n",
    "one_hot_train[np.arange(y_train.size), y_train ] = 1\n",
    "\n",
    "one_hot_test = np.zeros((y_test.size, len(set(y_test))), dtype=int)\n",
    "one_hot_test[np.arange(y_test.size), y_test ] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkaCDOGapMyl"
   },
   "source": [
    "## Modelo generativo\n",
    "\n",
    "Vamos a crear nuestro propio modelo generativo. En clase de teoría has visto muchas versiones distintas:\n",
    "\n",
    "1. Mezcla de distribuciones de Gaussianas (GMM)\n",
    "1. Mezcla de distribuciones multinomiales (Naive Bayes)\n",
    "1. Modelos de Markov ocultos (HMM)\n",
    "\n",
    "Tal y como se os apunta en teoría, los modelos generativos abordan un problema más general, y aprenden realmente cómo se estructuran y distribuyen los datos de entrada. \n",
    "\n",
    "En nuestro caso, vamos a distribuír los datos de entrada mediante el uso de **Autoencoders**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pxf_lSC1HsYh"
   },
   "source": [
    "# Autoencoders\n",
    "\n",
    "El autoencoder es un tipo de red que se utiliza para aprender codificaciones eficientes de datos sin etiquetar (lo que se conoce como aprendizaje no supervisado). Es una red que tiene el mismo tamaño en la entrada como en la salida, puesto que el objetivo de la red es reconstruír la entrada con la menor pérdida posible.\n",
    "\n",
    "Si lo que hacemos es reconstruír la entrada, ¿qué sentido tiene el usar la red? Habitualmente, **la red consta, a su mitad, de una capa con menos elementos que los datos de entrada**. Por tanto, al reconstruír los datos de la entrada a la salida, en esa capa tendremos una versión *comprimida* de la entrada, que contendrá la mayor parte de su información.\n",
    "\n",
    "Por tanto, podemos dividir un autoencoder en 3 secciones diferentes, tal y como se ve en la siguiente figura:\n",
    "\n",
    "![autoencoder](https://drive.google.com/uc?export=view&id=1yxkKZV0J0YplQAGPGJxQ2Z80Ad6L94eu)\n",
    "\n",
    "\n",
    "1. **Encoder:** es la parte inicial de la red, encargada de comprimir los datos de la entrada.\n",
    "1. **Code:** es la salida del encoder, contiene la versión *comprimida* de los datos de entrada.\n",
    "1. **Decoder:** se encarga de, partiendo de la salida del *Encoder*, reconstruír la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-mBCsDXJX3M"
   },
   "source": [
    "## Crea tu propio Autoencoder\n",
    "\n",
    "El diseño del autoencoder es libre (capas densas, convolucionales, ...), puedes crearlo como quieras. **El único requisito es que tiene que mantener los nombres (y parámetros) de las funciones descritas abajo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M95R6t1pJW3f"
   },
   "outputs": [],
   "source": [
    "# TODO: crea tu propio autoencoder\n",
    "\n",
    "\n",
    "class MiAutoencoder:\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        self.encoder = models.Sequential()\n",
    "        self.encoder.add(layers.InputLayer(shape=self.input_shape))\n",
    "        self.encoder.add(layers.Dense(512, activation='relu'))\n",
    "        self.encoder.add(layers.Dense(128, activation='relu'))\n",
    "        self.encoder.add(layers.Dense(64, activation='relu'))\n",
    "        self.encoder.add(layers.Dense(32, activation='relu'))\n",
    "        self.encoder.add(layers.Dense(32, activation='relu'))  # Latent space\n",
    "\n",
    "        self.decoder = models.Sequential()\n",
    "        self.decoder.add(layers.InputLayer(shape=(32,)))\n",
    "        self.decoder.add(layers.Dense(32, activation='relu'))\n",
    "        self.decoder.add(layers.Dense(64, activation='relu'))\n",
    "        self.decoder.add(layers.Dense(128, activation='relu'))\n",
    "        self.decoder.add(layers.Dense(512, activation='relu'))\n",
    "        self.decoder.add(layers.Dense(self.input_shape[0], activation='sigmoid'))  # Output should match input\n",
    "\n",
    "        self.autoencoder = models.Sequential([self.encoder, self.decoder])\n",
    "\n",
    "        self.autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    def fit(self, X, y=None, sample_weight=None, batch_size=60_000, epochs=20):\n",
    "        # TODO: entrena el modelo. Escoge el tamaño de batch y el número de epochs que quieras\n",
    "        \n",
    "        self.autoencoder.fit(X, X, \n",
    "                             batch_size=batch_size, \n",
    "                             epochs=epochs, \n",
    "                             sample_weight=sample_weight)\n",
    "\n",
    "    def get_encoded_data(self, X):\n",
    "        # TODO: devuelve la salida del encoder (code)\n",
    "        \n",
    "        return self.encoder.predict(X)\n",
    "        \n",
    "    def __del__(self):\n",
    "        # elimina todos los modelos que hayas creado\n",
    "        tf.keras.backend.clear_session() # Necesario para liberar la memoria en GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tt0L2yCMdmb"
   },
   "source": [
    "## Crea tu propio Clasificador\n",
    "\n",
    "El diseño del clasificador es libre, pero recuerda que tiene que ser simple (máximo dos capas). **El único requisito es que tiene que mantener los nombres (y parámetros) de las funciones descritas abajo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mh0yzbKMuhk"
   },
   "outputs": [],
   "source": [
    "# TODO: crea tu propio clasificador\n",
    "\n",
    "class MiClasificador:\n",
    "\n",
    "    def __init__(self):\n",
    "        # TODO : define el modelo y compílalo\n",
    "        \n",
    "        self.input_shape = (32,)\n",
    "        \n",
    "        self.classifier = models.Sequential()\n",
    "        self.classifier.add(layers.InputLayer(shape=self.input_shape))\n",
    "        self.classifier.add(layers.Dense(32, activation='relu'))\n",
    "        self.classifier.add(layers.Dense(10, activation='sigmoid'))\n",
    "\n",
    "        self.classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None, batch_size=60_000, epochs=15):\n",
    "        # TODO: entrena el modelo. Escoge el tamaño de batch y el número de epochs que quieras\n",
    "        self.classifier.fit(X, y, \n",
    "                             batch_size=batch_size, \n",
    "                             epochs=epochs, \n",
    "                             sample_weight=sample_weight)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO: devuelve la clase ganadora\n",
    "\n",
    "        return np.argmax(self.predict_proba(X)) + 1\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        return self.classifier.predict(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \n",
    "        return self.classifier.evaluate(X, y)[1]\n",
    "\n",
    "    def __del__(self):\n",
    "        # elimina todos los modelos que hayas creado\n",
    "        tf.keras.backend.clear_session() # Necesario para liberar la memoria en GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1-v4D6VH3Qq"
   },
   "source": [
    "### Entrenamiendo del modelo semisupervisado\n",
    "\n",
    "El entrenamiento del sistema semisupervisado se realiza en dos pasos.\n",
    "\n",
    "1. Se entrena el autoencoder con todos los datos (etiquetados y sin etiquetar).\n",
    "1. Se entrena un clasificador simple (una o dos capas), teniendo como entrada la salida del encoder (**code**) de los datos etiquetados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqT2nuCspfE_"
   },
   "source": [
    "<font color='red'>NOTA:</font> para entrenar (y predecir) vamos a utilizar los nombres de las funciones que hemos definido en el autoencoder y en el clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xjcLa21EKen"
   },
   "outputs": [],
   "source": [
    "# TODO: implementa el algoritmo semisupervised_training.\n",
    "\n",
    "def semisupervised_training(autoencoder, classifier, x_train, y_train, unlabeled_data):\n",
    "\n",
    "    all_x = np.vstack((x_train, unlabeled_train))\n",
    "    autoencoder.fit(all_x)\n",
    "    x_coded = autoencoder.get_encoded_data(x_train)\n",
    "    classifier.fit(x_coded, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjFXe6EiYfRg"
   },
   "source": [
    "### Entrenamos nuestro modelo\n",
    "\n",
    "Usa lo hecho anteriormente para entrenar tu clasificador de una manera semi-supervisada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNC1s2Wmqx4x"
   },
   "outputs": [],
   "source": [
    "# Crea tu autoencoder y tu clasificador\n",
    "\n",
    "autoencoder = MiAutoencoder(input_shape=x_train[0].shape)\n",
    "classifier = MiClasificador()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hN2zd3DEYnKI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Entrena tu modelo\n",
    "\n",
    "semisupervised_training(autoencoder=autoencoder, classifier=classifier, x_train=x_train, y_train=one_hot_train, unlabeled_data=unlabeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5tS8_SKOngm"
   },
   "outputs": [],
   "source": [
    "# TODO: Obtén la precisión sobre el conjunto de test\n",
    "pred_data = autoencoder.get_encoded_data(x_test)\n",
    "print('Test accuracy :', classifier.score(pred_data, one_hot_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbUKp14pPsrp"
   },
   "source": [
    "## Mejorando el código\n",
    "\n",
    "nuestro modelo actual requiere de dos pasos para entrenarse, pero podría realizarse en un único paso si **creamos un modelo con las dos salidas (autoencoder y clasificador)**. \n",
    "\n",
    "Para ello, hay que tener en cuenta que, en los datos sin etiquetar, su contribución al clasificador debería ser nula.\n",
    "\n",
    "\n",
    "### TRABAJO: Crea el nuevo modelo y modifica la función semisupervised_training para tener en cuenta todos los puntos mencionados anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xS3JLE37SqrG"
   },
   "outputs": [],
   "source": [
    "# TODO: crea el nuevo modelo\n",
    "\n",
    "# TODO: crea tu propio clasificador\n",
    "\n",
    "class MiClasificadorSemisupervisado:\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        # TODO : define el modelo y compílalo\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = 10\n",
    "        \n",
    "        input_layer = layers.Input(shape=self.input_shape)\n",
    "        \n",
    "        # Encoder part (shared for both autoencoder and classifier)\n",
    "        encoded = layers.Dense(128, activation='relu', kernel_regularizer='l2')(input_layer)\n",
    "        encoded = layers.Dense(64, activation='relu', kernel_regularizer='l2')(encoded)\n",
    "        encoded = layers.Dense(32, activation='relu', kernel_regularizer='l2')(encoded)\n",
    "        \n",
    "        # Decoder for autoencoder part\n",
    "        decoded = layers.Dense(64, activation='relu', kernel_regularizer='l2')(encoded)\n",
    "        decoded = layers.Dense(128, activation='relu', kernel_regularizer='l2')(decoded)\n",
    "        decoded = layers.Dense(self.input_shape[0], activation='sigmoid',name='autoencoder')(decoded)\n",
    "\n",
    "        # Classifier part\n",
    "        classifier = layers.Dense(32, activation='relu', kernel_regularizer='l2')(encoded)\n",
    "        classifier = layers.Dense(32, activation='relu', kernel_regularizer='l2')(classifier)\n",
    "        classifier_output = layers.Dense(self.num_classes, activation='softmax',name='classifier')(classifier)\n",
    "\n",
    "        # Autoencoder model (for reconstructing input)\n",
    "        self.autoencoder = models.Model(input_layer, decoded)\n",
    "        self.autoencoder.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        # Classifier model (for predicting class labels)\n",
    "        self.classifier = models.Model(input_layer, classifier_output)\n",
    "        self.classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Combined model with two outputs: one for autoencoder (reconstruction) and one for classifier (classification)\n",
    "        self.model = models.Model(input_layer, \n",
    "                                  [decoded, classifier_output])\n",
    "                                  #classifier_output)\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss=['mse', 'categorical_crossentropy'],\n",
    "                           #loss='categorical_crossentropy',\n",
    "                           loss_weights=[.5, 1.5],  # Adjust loss weights if needed\n",
    "                           metrics=['accuracy', 'accuracy'])\n",
    "    \n",
    "    def fit(self, X, y, unlabeled_data, batch_size,  epochs):\n",
    "        # TODO: entrena el modelo. Escoge el tamaño de batch y el número de epochs que quieras, y define bien el sample_weight\n",
    "\n",
    "        all_x = np.vstack((X, unlabeled_train))\n",
    "        y_zeros = np.zeros((unlabeled_data.shape[0],y.shape[1]))\n",
    "        all_y = np.vstack((y,y_zeros))\n",
    "        weight_autoencoder = np.ones(len(all_x))\n",
    "        weight_classifier = np.array([1]*len(X) + [0]*len(unlabeled_data))\n",
    "        \n",
    "        h = self.model.fit(all_x, \n",
    "                       [all_x, all_y], \n",
    "                       #all_y,\n",
    "                       sample_weight=[weight_autoencoder, weight_classifier], \n",
    "                       #sample_weight=sample_weight,\n",
    "                       epochs=epochs, \n",
    "                       batch_size=batch_size, \n",
    "                       verbose=1)\n",
    "        return h\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO: devuelve la clase ganadora del clasificador\n",
    "        _, predictions = self.model.predict(X)\n",
    "        return predictions.argmax(axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # TODO: devuelve la probabilidad del clasificador\n",
    "        _, predictions = self.model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n",
    "\n",
    "    def __del__(self):\n",
    "        # elimina todos los modelos que hayas creado\n",
    "        tf.keras.backend.clear_session() # Necesario para liberar la memoria en GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eF_9LMeZ2J2"
   },
   "outputs": [],
   "source": [
    "# TODO: reescribe la función semisupervised_training para incorporar las mejoras mencionadas anteriormente\n",
    "\n",
    "model = MiClasificadorSemisupervisado(input_shape=x_train[0].shape)\n",
    "\n",
    "def semisupervised_training_v2(model, x_train, y_train, unlabeled_data):\n",
    "    h = model.fit(x_train, y_train, unlabeled_data, batch_size=20_000, epochs = 100)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbqC0inexwHp"
   },
   "outputs": [],
   "source": [
    "# TODO: Crea y entrena tu clasificador\n",
    "\n",
    "model = MiClasificadorSemisupervisado(input_shape=x_train[0].shape)\n",
    "\n",
    "h = semisupervised_training_v2(model, x_train, one_hot_train, unlabeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSVVW8fZXWGs"
   },
   "outputs": [],
   "source": [
    "# TODO: Obtén la precisión sobre el conjunto de test\n",
    "print('Test accuracy :', model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPGRXmHXptYW"
   },
   "source": [
    "# Hay vida más allá del autoencoder\n",
    "\n",
    "¿Has probado a utilizar otro método distinto del autoencoder para obtener una respresentación similar a la salida del encoder? La idea es la siguiente:\n",
    "\n",
    "1. Define un modelo $model$ convolucional similar al encoder de un autoencoder (la entrada es el tamaño de la imagen, la salida el vector de representación)\n",
    "1. Define una capa de salida $cluster$ que, partiendo de la salida de model, nos devuelva una salida con el mismo número de clases que el dataset a utilizar (la entrada es el vector de representación), usando softmax como activación de salida\n",
    "1. Para cada batch de entrenamiento $X$:  # Usa un batch alto, mínimo 128\n",
    "  1. Modifica las imágenes de entrada con [data_augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation?hl=es-419), llámala $augX_1$.\n",
    "  1. Modifica otra vez las imágenes de entrada con [data_augmentation_2](https://www.tensorflow.org/tutorials/images/data_augmentation?hl=es-419), llámala $augX_2$.\n",
    "  1. $augX_{1comp} \\leftarrow model(augX_1)$\n",
    "  1. $augX_{2comp} \\leftarrow model(augX_2)$\n",
    "  1. $cX_{1comp} \\leftarrow cluster(augX_{1comp})$\n",
    "  1. $cX_{2comp} \\leftarrow cluster(augX_{2comp})$\n",
    "  1. $M \\leftarrow augX_{1comp} ~ augX_{2comp}^T$\n",
    "  1. $loss_C \\leftarrow cX_{1comp}(1 - cX_{1comp}) + cX_{2comp}(1 - cX_{2comp})$ # Puede que tengas que crear tu [propia función de coste](https://keras.io/api/losses/#creating-custom-losses)\n",
    "  1. $loss_M \\leftarrow crossentropy(I, softmax(M/\\tau, axis=1)))$ # Puede que tengas que crear tu [propia función de coste](https://keras.io/api/losses/#creating-custom-losses)\n",
    "  1. $\\tau$ es un hiperparámetro que se suele definir a 5.0\n",
    "  1. $loss \\leftarrow loss_M + \\lambda~loss_C$\n",
    "    1. $\\lambda$ es un hiperparámetro (puedes probar con 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cXegrUWtiFW"
   },
   "outputs": [],
   "source": [
    "# Escribe aquí la solución. Crea tantos bloques de código como necesites. Puedes utilizar la siguiente red para generar distorsiones\n",
    "\n",
    "data_augmentation = tf.keras.models.Sequential(\n",
    "    [\n",
    "        # tf.keras.layers.RandomFlip(\"horizontal\"),  # Puede ser util en otros casos\n",
    "        tf.keras.layers.RandomRotation(0.05),\n",
    "        tf.keras.layers.RandomTranslation(0.15, 0.15),\n",
    "        tf.keras.layers.RandomZoom(.15),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_augmentation_2 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        # tf.keras.layers.RandomFlip(\"horizontal\"),  # Puede ser util en otros casos\n",
    "        tf.keras.layers.RandomTranslation(0.15, 0.15),\n",
    "        tf.keras.layers.Resizing(48, 48), # para CIFAR, para MNIST usar 40 en lugar de 48\n",
    "        tf.keras.layers.RandomCrop(32, 32), # para CIFAR, para MNIST usar 28 en lugar de 32\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEZfd7iVX94s"
   },
   "source": [
    "# Trabajo extra\n",
    "\n",
    "¿Has probado a hacer el autoencoder totalmente convolucional? Para el *decoder* puedes usar las funciones [UpSampling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D) o [Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(h.history['classifier_accuracy']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ma2Python12",
   "language": "python",
   "name": "ma2python12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
