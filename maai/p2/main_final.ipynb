{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pablo Chantada Saborido (pablo.chantada@udc.es)\n",
    "### José Romero Conde (j.rconde@udc.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import DatasetProcess, reconstruction_plot, anomaly_report, plot_atipicos\n",
    "from ConvModel import ConvModel\n",
    "from AutoEncoder import TwoStepAutoEncoder, TwoStepClassifier, TwoStepTraining, OneStepAutoencoder, OneStepTraining\n",
    "from OneClass import AnomalyDetector\n",
    "from Contrastive import ContrastiveModel, SemiSupervisedContrastiveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_train, x_train, y_train, x_val, y_val, x_test, y_test, one_hot_train, one_hot_val, one_hot_test = DatasetProcess.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos no etiquetados: (33500, 32, 32, 3)\n",
      "Datos etiquetados entrenamiento: (8250, 32, 32, 3)\n",
      "Etiquetas entrenamiento: (8250, 1)\n",
      "Datos validación: (8250, 32, 32, 3)\n",
      "Etiquetas validación: (8250, 1)\n",
      "Datos prueba: (10000, 32, 32, 3)\n",
      "Etiquetas prueba: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#(x_train, y_train), (x_test, y_test) = DatasetProcess.load()\n",
    "\n",
    "\n",
    "# Aplicar la función hold_out\n",
    "#(x_train_no_labeled, x_train_labeled, y_train_labeled), (x_val, y_val), (x_test, y_test) = DatasetProcess.hold_out(\n",
    "#    (x_train, y_train), (x_test, y_test), validation_size=1000\n",
    "#)\n",
    "\n",
    "#x_train_labeled = x_train_labeled.astype('float32') / 255.0\n",
    "#x_val = x_val.astype('float32') / 255.0\n",
    "#x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Verificar las dimensiones\n",
    "print(f\"Datos no etiquetados: {unlabeled_train.shape}\")\n",
    "print(f\"Datos etiquetados entrenamiento: {x_train.shape}\")\n",
    "print(f\"Etiquetas entrenamiento: {y_train.shape}\")\n",
    "print(f\"Datos validación: {x_val.shape}\")\n",
    "print(f\"Etiquetas validación: {y_val.shape}\")\n",
    "print(f\"Datos prueba: {x_test.shape}\")\n",
    "print(f\"Etiquetas prueba: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO 1\n",
    "\n",
    "Entrena un modelo, creado sobre TensorFlow, haciendo uso únicamente de las instancias etiquetadas de entrenamiento. Dicho modelo debe de tener al menos cuatro capas densas y/o convolucionales.\n",
    "\n",
    "Responde a las siguientes preguntas:\n",
    "\n",
    "1. ¿Qué red has escogido? ¿Por qué? ¿Cómo la has entrenado?\n",
    "2. ¿Cuál es el rendimiento del modelo en entrenamiento? ¿Y en prueba?\n",
    "3. ¿Qué conclusiones sacas de los resultados detallados en el punto anterior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_base = ConvModel()\n",
    "history_base = model_base.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    batch_size=128,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalúa con el conjunto de prueba\n",
    "test_accuracy = model_base.score(x_test, y_test)\n",
    "print(f\"Accuracy en conjunto de prueba: {test_accuracy}\")\n",
    "\n",
    "model_base.plot(history_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO 2\n",
    "\n",
    "Entrena el mismo modelo, incorporando las instancias no etiquetadas de entrenamiento mediante la técnica de auto-aprendizaje. Opcionalmente, se ponderará cada instancia de entrada en función de su calidad (o certeza).\n",
    "\n",
    "Responde a las siguientes preguntas:\n",
    "1. ¿Qué parámetros has definido para el entrenamiento?\n",
    "2. ¿Cuál es el rendimiento del modelo en entrenamiento? ¿Y en prueba?\n",
    "3. ¿Se mejoran los resultados obtenidos en el Ejercicio 1?\n",
    "4. ¿Qué conclusiones sacas de los resultados detallados en los puntos anteriores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear modelos consistentes durante self-training\n",
    "def create_model():\n",
    "    return ConvModel(\n",
    "        learning_rate=0.0005,  # Learning rate reducido para fine-tuning\n",
    "        dropout_prob=0.25,     \n",
    "        l2_lambda=0.005        \n",
    "    )\n",
    "\n",
    "# Aplica self-training con datos no etiquetados\n",
    "final_model = ConvModel.self_training_v2(\n",
    "    model_func=create_model,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,  \n",
    "    unlabeled_data=unlabeled_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    thresh=0.8,             \n",
    "    train_epochs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evalúa el modelo final\n",
    "final_accuracy = final_model.score(x_test, y_test)\n",
    "print(f\"Accuracy del modelo final con self-training: {final_accuracy}\")\n",
    "print(f\"Mejora respecto al modelo base: {final_accuracy - test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO 3\n",
    "\n",
    "Entrena un modelo de aprendizaje semisupervisado de tipo autoencoder en dos pasos (primero el autoencoder, después el clasificador). La arquitectura del encoder debe ser exactamente la misma que la definida en los Ejercicios 1 y 2, a excepción del último bloque de capas.\n",
    "\n",
    "Responde a las siguientes preguntas:\n",
    "1. ¿Cuál es la arquitectura del modelo? ¿Y sus hiperparámetros?\n",
    "2. ¿Cuál es el rendimiento del modelo en entrenamiento? ¿Y en prueba?\n",
    "3. ¿Se mejoran los resultados obtenidos en los Ejercicios 1 y 2?\n",
    "4. ¿Qué conclusiones sacas de los resultados detallados en los puntos anteriores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = TwoStepAutoEncoder(\n",
    "                                input_shape=unlabeled_train[0].shape,\n",
    "                                learning_rate=0.006,\n",
    "                                l2_lambda=0.0005,\n",
    "                                dropout_prob=0.1)\n",
    "classifier = TwoStepClassifier(\n",
    "                              l2_lambda=0.0005,\n",
    "                              dropout_prob=0.05,\n",
    "                               learning_rate=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TwoStepTraining(autoencoder=autoencoder, \n",
    "                classifier=classifier, \n",
    "                x_train=x_train, \n",
    "                y_train=one_hot_train, \n",
    "                unlabeled_train=unlabeled_train, \n",
    "                validation_data=(x_val, one_hot_val),\n",
    "                batch_size_autoencoder=256,\n",
    "                epochs_autoencoder=1,\n",
    "                batch_size_classifier=4096,\n",
    "                epochs_classifier=405)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_plot(autoencoder, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.score(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO 4\n",
    "\n",
    "Entrena un modelo de aprendizaje semisupervisado de tipo autoencoder en un paso (autoencoder y clasificador al mismo tiempo). La arquitectura del autoencoder será la misma que la definida en el Ejercicio 3, y la combinación de encoder y clasificador será igual a la arquitectura definida en el\n",
    "Ejercicio 1.\n",
    "\n",
    "Responde a las siguientes preguntas:\n",
    "1. ¿Cuál es la arquitectura del modelo? ¿Y sus hiperparámetros?\n",
    "2. ¿Cuál es el rendimiento del modelo en entrenamiento? ¿Y en prueba?\n",
    "3. ¿Se mejoran los resultados obtenidos en los ejercicios anteriores?\n",
    "4. ¿Qué conclusiones sacas de los resultados detallados en los puntos anteriores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 10:57:36.985038: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-04-04 10:57:36.985115: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-04-04 10:57:36.985130: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-04-04 10:57:36.985177: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-04 10:57:36.985209: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "one_step_autoencoder = OneStepAutoencoder(input_shape=unlabeled_train[0].shape,\n",
    "                                learning_rate=0.0015,\n",
    "                                decoder_extra_loss_weight = 0.45,\n",
    "                                l2_lambda=0.00005,\n",
    "                                dropout_prob=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 10:57:41.745475: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 3s/step - classifier_accuracy: 0.0238 - classifier_loss: 0.9097 - decoder_loss: 0.0601 - loss: 0.6990\n"
     ]
    }
   ],
   "source": [
    "h = OneStepTraining(one_step_autoencoder, \n",
    "                    x_train=x_train, \n",
    "                    y_train=one_hot_train, \n",
    "                    unlabeled_train=unlabeled_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=1,\n",
    "                    patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 643ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reconstruction_plot(one_step_autoencoder, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_step_autoencoder.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO 5\n",
    "\n",
    "Repite el mismo entrenamiento de los Ejercicios 1-4, pero eliminando las instancias no etiquetadas\n",
    "más atípicas con respecto a los datos etiquetados. Se cumplirán los siguientes puntos:\n",
    "- La arquitectura de la red de clasificación en una clase será la misma a la utilizada en el\n",
    "clasificador del Ejercicio 1, a excepción de la capa de salida.\n",
    "- Utiliza la técnica explicada en el Notebook 5, usando un valor de 𝑣 = 0,9.\n",
    "\n",
    "Responde a la siguiente pregunta:\n",
    "1. ¿Se mejoran los resultados con respecto a los anteriores ejercicios? ¿Qué conclusiones sacas de estos resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pepe/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2025-04-03 23:44:05.002775: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-04-03 23:44:05.003053: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-04-03 23:44:05.003085: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-04-03 23:44:05.003349: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-03 23:44:05.003403: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/Users/pepe/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 23:44:06.559175: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step- loss: 0.3736 \n",
      "Cambiando r a 0.69512767 , max: 0.70241475 , min: 0.6930452\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 656ms/step - loss: 0.3712\n",
      "Epoch 2/2\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step- loss: 0.0720 \n",
      "Cambiando r a 0.6791968 , max: 0.69847983 , min: 0.67324144\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 618ms/step - loss: 0.0717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential, built=True>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalyDetector = AnomalyDetector(input_shape=(32,32,3), \n",
    "                        nu=.9,\n",
    "                        l2_lambda=0.0,\n",
    "                        learning_rate=0.0001,\n",
    "                        dropout_prob=0.0)\n",
    "anomalyDetector.fit(x_train, \n",
    "          batch_size=256, \n",
    "          epochs=2, \n",
    "          delta=.025, \n",
    "          steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_unlabeled_train, is_typical = anomaly_report(anomalyDetector, unlabeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo que va pero no estoy seguro\n",
    "#\n",
    "# plot_atipicos(filtered_unlabeled_train, is_typical, unlabeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 2 con datos filtrados\n",
    "model_self_filtered = ConvModel.self_training_v2(\n",
    "    model_func=create_model,\n",
    "    x_train=x_train_labeled,\n",
    "    y_train=y_train_labeled,\n",
    "    unlabeled_data=filtered_unlabeled_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    thresh=0.8,\n",
    "    train_epochs=1\n",
    ")\n",
    "\n",
    "# Evaluar los modelos filtrados\n",
    "test_accuracy_self_filtered = model_self_filtered.score(x_test, y_test)\n",
    "print(f\"Accuracy en conjunto de prueba (self-training filtrado): {test_accuracy_self_filtered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 3 con datos filtrados\n",
    "autoencoder_filtered = TwoStepAutoEncoder(\n",
    "    input_shape=filtered_unlabeled_train[0].shape,\n",
    "    learning_rate=0.006,\n",
    "    l2_lambda=0.0005,\n",
    "    dropout_prob=0.1\n",
    ")\n",
    "\n",
    "classifier_filtered = TwoStepClassifier(\n",
    "    l2_lambda=0.0005,\n",
    "    dropout_prob=0.05,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "\n",
    "history_two_step_filtered = TwoStepTraining(\n",
    "    autoencoder=autoencoder_filtered, \n",
    "    classifier=classifier_filtered, \n",
    "    x_train=x_train, \n",
    "    y_train=one_hot_train, \n",
    "    unlabeled_train=filtered_unlabeled_train, \n",
    "    validation_data=(x_val, one_hot_val),\n",
    "    batch_size_autoencoder=256,\n",
    "    epochs_autoencoder=50,\n",
    "    batch_size_classifier=256,\n",
    "    epochs_classifier=100\n",
    ")\n",
    "\n",
    "# Evaluar modelos\n",
    "test_accuracy_two_step_filtered = classifier_filtered.score(x_test, y_test)  # No se si es asi como se evaluaria este la verdad supongo que si\n",
    "print(f\"Accuracy del modelo one-step filtrado: {test_accuracy_two_step_filtered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 4 con datos filtrados\n",
    "one_step_autoencoder_filtered = OneStepAutoencoder(\n",
    "    input_shape=filtered_unlabeled_train[0].shape,\n",
    "    learning_rate=0.0015,\n",
    "    decoder_extra_loss_weight=0.45,\n",
    "    l2_lambda=0.00005,\n",
    "    dropout_prob=0.05\n",
    ")\n",
    "\n",
    "history_one_step_filtered = OneStepTraining(\n",
    "    one_step_autoencoder_filtered, \n",
    "    x_train=x_train, \n",
    "    y_train=one_hot_train, \n",
    "    unlabeled_train=filtered_unlabeled_train,\n",
    "    batch_size=256,\n",
    "    epochs=100,\n",
    "    patience=10\n",
    ")\n",
    "# Evaluar modelos\n",
    "test_accuracy_one_step_filtered = one_step_autoencoder_filtered.score(x_test, y_test)\n",
    "print(f\"Accuracy del modelo one-step filtrado: {test_accuracy_one_step_filtered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO 6\n",
    "\n",
    "Repite los Ejercicios 3-5 cambiando el autencoder por la técnica definida en el apartado “Hay vida más allá del autoencoder” del Notebook 4. Contesta a las preguntas de dichos ejercicios. Se cumplirán los siguientes puntos:\n",
    "\n",
    "1. La arquitectura de la red será igual a la parte encoder del autencoder definido en los\n",
    "ejercicios anteriores.\n",
    "2. El modelo debe entrenar correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejercicio 3 two step con todos los datos\n",
    "\n",
    "cModel = ContrastiveModel(unlabeled_train[0].shape, \n",
    "                          learning_rate=0.05, \n",
    "                          lambda_param=.9,\n",
    "                          l2_lambda=0.001,\n",
    "                          dropout_prob=0.001)\n",
    "classifier = TwoStepClassifier(\n",
    "                              l2_lambda=0.0005,\n",
    "                              dropout_prob=0.05,\n",
    "                               learning_rate=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................................Epoch 1/20, Total Loss: 5.1303, Contrastive Loss: 4.9989, Clustering Loss: 0.1461\n",
      "..................................................................................................................................Epoch 2/20, Total Loss: 4.5734, Contrastive Loss: 4.5734, Clustering Loss: 0.0000\n",
      ".........................................."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/var/folders/1l/sdrz_tw104ld922_w0xnl5840000gn/T/ipykernel_2703/2410891534.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m TwoStepTraining(autoencoder=cModel, \n\u001b[32m      2\u001b[39m                 classifier=classifier,\n\u001b[32m      3\u001b[39m                 x_train=x_train,\n\u001b[32m      4\u001b[39m                 y_train=one_hot_train,\n",
      "\u001b[32m~/repositorios/3year2sem/maai/p2/AutoEncoder.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(autoencoder, classifier, x_train, y_train, unlabeled_train, validation_data, batch_size_autoencoder, epochs_autoencoder, batch_size_classifier, epochs_classifier, contrastive)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m TwoStepTraining(autoencoder, classifier, x_train, y_train, unlabeled_train, validation_data=\u001b[38;5;28;01mNone\u001b[39;00m, batch_size_autoencoder=\u001b[32m1024\u001b[39m, epochs_autoencoder=\u001b[32m100\u001b[39m, batch_size_classifier=\u001b[32m1024\u001b[39m, epochs_classifier=\u001b[32m100\u001b[39m, contrastive=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    158\u001b[39m \n\u001b[32m    159\u001b[39m     all_x = np.vstack((x_train, unlabeled_train))\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m contrastive:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m         autoencoder.train(unlabeled_train, epochs=epochs_autoencoder, batch_size= batch_size_autoencoder)\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    163\u001b[39m         autoencoder.fit(all_x, batch_size=batch_size_autoencoder, epochs=epochs_autoencoder, validation_data=(validation_data[\u001b[32m0\u001b[39m],validation_data[\u001b[32m0\u001b[39m]))\n\u001b[32m    164\u001b[39m     x_coded = autoencoder.get_encoded_data(x_train)\n",
      "\u001b[32m~/repositorios/3year2sem/maai/p2/Contrastive.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dataset, epochs, batch_size, temperature)\u001b[39m\n\u001b[32m    182\u001b[39m             batch_count = \u001b[32m0\u001b[39m\n\u001b[32m    183\u001b[39m \n\u001b[32m    184\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;28;01min\u001b[39;00m self.mini_batches(dataset, batch_size=batch_size):\n\u001b[32m    185\u001b[39m                 print(\u001b[33m'.'\u001b[39m,end=\u001b[33m''\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m                 loss_dict = self.train_step(data, temperature=temperature)\n\u001b[32m    187\u001b[39m                 epoch_total_loss += loss_dict[\u001b[33m\"loss\"\u001b[39m]\n\u001b[32m    188\u001b[39m                 epoch_contrastive_loss += loss_dict[\u001b[33m\"contrastive_loss\"\u001b[39m]\n\u001b[32m    189\u001b[39m                 epoch_clustering_loss += loss_dict[\u001b[33m\"clustering_loss\"\u001b[39m]\n",
      "\u001b[32m~/repositorios/3year2sem/maai/p2/Contrastive.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, data, temperature)\u001b[39m\n\u001b[32m    154\u001b[39m \n\u001b[32m    155\u001b[39m         \u001b[38;5;66;03m# Calcular gradientes y actualizar pesos\u001b[39;00m\n\u001b[32m    156\u001b[39m         gradients = tape.gradient(total_loss, self.encoder.trainable_variables + self.cluster.trainable_variables)\n\u001b[32m    157\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         grad_norm = tf.linalg.global_norm(gradients)\n\u001b[32m    159\u001b[39m \n\u001b[32m    160\u001b[39m         self.optimicer.apply_gradients(zip(gradients, self.encoder.trainable_variables + self.cluster.trainable_variables))\n\u001b[32m    161\u001b[39m \n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/util/dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/ops/clip_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(t_list, name)\u001b[39m\n\u001b[32m    284\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(v):\n\u001b[32m    285\u001b[39m           half_squared_norms.append(gen_nn_ops.l2_loss(v))\n\u001b[32m    286\u001b[39m \n\u001b[32m    287\u001b[39m     half_squared_norm = math_ops.reduce_sum(\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m         array_ops_stack.stack(half_squared_norms))\n\u001b[32m    289\u001b[39m \n\u001b[32m    290\u001b[39m     norm = math_ops.sqrt(\n\u001b[32m    291\u001b[39m         half_squared_norm *\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/util/dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/ops/array_ops_stack.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(values, axis, name)\u001b[39m\n\u001b[32m     71\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     73\u001b[39m       \u001b[38;5;66;03m# If the input is a constant list, it can be converted to a constant op\u001b[39;00m\n\u001b[32m     74\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m ops.convert_to_tensor(values, name=name)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError, NotImplementedError):\n\u001b[32m     76\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Input list contains non-constant tensors\u001b[39;00m\n\u001b[32m     77\u001b[39m \n\u001b[32m     78\u001b[39m   value_shape = ops.convert_to_tensor(values[\u001b[32m0\u001b[39m], name=name)._shape_tuple()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/profiler/trace.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m wrapped(*args, **kwargs):\n\u001b[32m    180\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m enabled:\n\u001b[32m    181\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, **trace_kwargs):\n\u001b[32m    182\u001b[39m           \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[39m\n\u001b[32m    709\u001b[39m ) -> Union[EagerTensor, SymbolicTensor]:\n\u001b[32m    710\u001b[39m   \u001b[33m\"\"\"Implementation of the public convert_to_tensor.\"\"\"\u001b[39m\n\u001b[32m    711\u001b[39m   \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[32m    712\u001b[39m   preferred_dtype = preferred_dtype \u001b[38;5;28;01mor\u001b[39;00m dtype_hint\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m   return tensor_conversion_registry.convert(\n\u001b[32m    714\u001b[39m       value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n\u001b[32m    715\u001b[39m   )\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    230\u001b[39m                   f\"actual = {ret.dtype.base_dtype.name}\",\n\u001b[32m    231\u001b[39m                   name=name))\n\u001b[32m    232\u001b[39m \n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n\u001b[32m    235\u001b[39m \n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;28;01mis\u001b[39;00m NotImplemented:\n\u001b[32m    237\u001b[39m       \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/ops/array_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(v, dtype, name, as_ref)\u001b[39m\n\u001b[32m   1301\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1302\u001b[39m     dtype = inferred_dtype\n\u001b[32m   1303\u001b[39m   \u001b[38;5;28;01melif\u001b[39;00m dtype != inferred_dtype:\n\u001b[32m   1304\u001b[39m     v = nest.map_structure(_cast_nested_seqs_to_dtype(dtype), v)\n\u001b[32m-> \u001b[39m\u001b[32m1305\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _autopacking_helper(v, dtype, name \u001b[38;5;28;01mor\u001b[39;00m \u001b[33m\"packed\"\u001b[39m)\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/ops/array_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(list_or_tuple, dtype, name)\u001b[39m\n\u001b[32m   1208\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m context.executing_eagerly():\n\u001b[32m   1209\u001b[39m     \u001b[38;5;66;03m# NOTE: Fast path when all the items are tensors, this doesn't do any type\u001b[39;00m\n\u001b[32m   1210\u001b[39m     \u001b[38;5;66;03m# checking.\u001b[39;00m\n\u001b[32m   1211\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m all(isinstance(elem, core.Tensor) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;28;01min\u001b[39;00m list_or_tuple):\n\u001b[32m-> \u001b[39m\u001b[32m1212\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m gen_array_ops.pack(list_or_tuple, name=name)\n\u001b[32m   1213\u001b[39m   must_pack = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1214\u001b[39m   converted_elems = []\n\u001b[32m   1215\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(name) \u001b[38;5;28;01mas\u001b[39;00m scope:\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(values, axis, name)\u001b[39m\n\u001b[32m   6715\u001b[39m         _ctx, \u001b[33m\"Pack\"\u001b[39m, name, values, \u001b[33m\"axis\"\u001b[39m, axis)\n\u001b[32m   6716\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   6717\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   6718\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m-> \u001b[39m\u001b[32m6719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m   6720\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   6721\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   6722\u001b[39m       return pack_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "TwoStepTraining(autoencoder=cModel, \n",
    "                classifier=classifier, \n",
    "                x_train=x_train, \n",
    "                y_train=one_hot_train, \n",
    "                unlabeled_train=unlabeled_train, \n",
    "                validation_data=(x_val, one_hot_val),\n",
    "                batch_size_autoencoder=256,\n",
    "                epochs_autoencoder=20,\n",
    "                batch_size_classifier=4096,\n",
    "                epochs_classifier=20, \n",
    "                contrastive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 1/35, Total Loss: 8.1865, Contrastive Loss: 4.1832, Clustering Loss: 0.0279, Supervised Loss: 3.9782\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 2/35, Total Loss: 7.8341, Contrastive Loss: 3.8475, Clustering Loss: 0.0000, Supervised Loss: 3.9866\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 3/35, Total Loss: 7.8146, Contrastive Loss: 3.8226, Clustering Loss: 0.0000, Supervised Loss: 3.9920\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 4/35, Total Loss: 7.8030, Contrastive Loss: 3.8189, Clustering Loss: 0.0000, Supervised Loss: 3.9841\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 5/35, Total Loss: 7.8038, Contrastive Loss: 3.8162, Clustering Loss: 0.0000, Supervised Loss: 3.9877\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 6/35, Total Loss: 7.8018, Contrastive Loss: 3.8174, Clustering Loss: 0.0000, Supervised Loss: 3.9844\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 7/35, Total Loss: 7.8015, Contrastive Loss: 3.8188, Clustering Loss: 0.0000, Supervised Loss: 3.9826\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 8/35, Total Loss: 7.8045, Contrastive Loss: 3.8190, Clustering Loss: 0.0000, Supervised Loss: 3.9855\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 9/35, Total Loss: 7.8076, Contrastive Loss: 3.8218, Clustering Loss: 0.0000, Supervised Loss: 3.9859\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 10/35, Total Loss: 7.8073, Contrastive Loss: 3.8200, Clustering Loss: 0.0000, Supervised Loss: 3.9873\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 11/35, Total Loss: 7.8066, Contrastive Loss: 3.8193, Clustering Loss: 0.0000, Supervised Loss: 3.9873\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 12/35, Total Loss: 7.8082, Contrastive Loss: 3.8202, Clustering Loss: 0.0000, Supervised Loss: 3.9881\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 13/35, Total Loss: 7.8053, Contrastive Loss: 3.8187, Clustering Loss: 0.0000, Supervised Loss: 3.9866\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 14/35, Total Loss: 7.8038, Contrastive Loss: 3.8190, Clustering Loss: 0.0000, Supervised Loss: 3.9848\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 15/35, Total Loss: 7.8051, Contrastive Loss: 3.8181, Clustering Loss: 0.0000, Supervised Loss: 3.9870\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 16/35, Total Loss: 7.8061, Contrastive Loss: 3.8188, Clustering Loss: 0.0000, Supervised Loss: 3.9873\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 17/35, Total Loss: 7.8066, Contrastive Loss: 3.8207, Clustering Loss: 0.0000, Supervised Loss: 3.9859\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 18/35, Total Loss: 7.8054, Contrastive Loss: 3.8188, Clustering Loss: 0.0000, Supervised Loss: 3.9866\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 19/35, Total Loss: 7.8035, Contrastive Loss: 3.8191, Clustering Loss: 0.0000, Supervised Loss: 3.9844\n",
      ".........................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "# ejercicio 4 ONE STEP con todos los datos\n",
    "\n",
    "cSSLModel = SemiSupervisedContrastiveModel(unlabeled_train[0].shape, \n",
    "                          learning_rate=0.1, \n",
    "                          lambda_param=.9,\n",
    "                          l2_lambda=0.0001,\n",
    "                          dropout_prob=0.001,\n",
    "                          lambda_supervised=1.0)\n",
    "\n",
    "\n",
    "cSSLModel.train(\n",
    "    X_unlabeled=unlabeled_train,  \n",
    "    X_labeled=x_train,        \n",
    "    y_labeled=y_train,       \n",
    "    epochs=35,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "cSSLModel.plot_training_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "test_samples = x_test[:n_samples].reshape(n_samples,32,32,3)\n",
    "\n",
    "cSSLModel.plot_similarity_matrix(test_samples, n_samples=n_samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejercicio 3 two step con los datos filtrados\n",
    "\n",
    "cModel = ContrastiveModel(unlabeled_train[0].shape, \n",
    "                          learning_rate=0.05, \n",
    "                          lambda_param=.9,\n",
    "                          l2_lambda=0.001,\n",
    "                          dropout_prob=0.001)\n",
    "classifier = TwoStepClassifier(\n",
    "                              l2_lambda=0.0005,\n",
    "                              dropout_prob=0.05,\n",
    "                               learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoStepTraining(autoencoder=cModel, \n",
    "                classifier=classifier, \n",
    "                x_train=x_train, \n",
    "                y_train=one_hot_train, \n",
    "                unlabeled_train=filtered_unlabeled_train, # <-\n",
    "                validation_data=(x_val, one_hot_val),\n",
    "                batch_size_autoencoder=256,\n",
    "                epochs_autoencoder=1,\n",
    "                batch_size_classifier=4096,\n",
    "                epochs_classifier=1, \n",
    "                contrastive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejercicio 4 ONE STEP con los datos filtrados\n",
    "\n",
    "cSSLModel = SemiSupervisedContrastiveModel(unlabeled_train[0].shape, \n",
    "                          learning_rate=0.05, \n",
    "                          lambda_param=.9,\n",
    "                          l2_lambda=0.001,\n",
    "                          dropout_prob=0.001,\n",
    "                          lambda_supervised=1.0)\n",
    "\n",
    "\n",
    "cSSLModel.train(\n",
    "    X_unlabeled=filtered_unlabeled_train,  # <-\n",
    "    X_labeled=x_train,        \n",
    "    y_labeled=y_train,       \n",
    "    epochs=6,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "cSSLModel.plot_training_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "test_samples = x_test[:n_samples].reshape(n_samples,32,32,3)\n",
    "\n",
    "cSSLModel.plot_similarity_matrix(test_samples, n_samples=n_samples);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma2Python12",
   "language": "python",
   "name": "ma2python12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
