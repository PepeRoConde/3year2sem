{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pablo Chantada Saborido (pablo.chantada@udc.es)\n",
    "### Jos√© Romero Conde (j.rconde@udc.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import DatasetProcess, reconstruction_plot, anomaly_report, plot_atipicos\n",
    "from ConvModel import ConvModel\n",
    "from AutoEncoder import TwoStepAutoEncoder, TwoStepClassifier, TwoStepTraining, OneStepAutoencoder, OneStepTraining\n",
    "from OneClass import AnomalyDetector\n",
    "from Contrastive import ContrastiveModel, SemiSupervisedContrastiveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_train, x_train, y_train, x_val, y_val, x_test, y_test, one_hot_train, one_hot_val, one_hot_test = DatasetProcess.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos no etiquetados: (33500, 32, 32, 3)\n",
      "Datos etiquetados entrenamiento: (8250, 32, 32, 3)\n",
      "Etiquetas entrenamiento: (8250, 1)\n",
      "Datos validaci√≥n: (8250, 32, 32, 3)\n",
      "Etiquetas validaci√≥n: (8250, 1)\n",
      "Datos prueba: (10000, 32, 32, 3)\n",
      "Etiquetas prueba: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#(x_train, y_train), (x_test, y_test) = DatasetProcess.load()\n",
    "\n",
    "\n",
    "# Aplicar la funci√≥n hold_out\n",
    "#(x_train_no_labeled, x_train_labeled, y_train_labeled), (x_val, y_val), (x_test, y_test) = DatasetProcess.hold_out(\n",
    "#    (x_train, y_train), (x_test, y_test), validation_size=1000\n",
    "#)\n",
    "\n",
    "#x_train_labeled = x_train_labeled.astype('float32') / 255.0\n",
    "#x_val = x_val.astype('float32') / 255.0\n",
    "#x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Verificar las dimensiones\n",
    "print(f\"Datos no etiquetados: {unlabeled_train.shape}\")\n",
    "print(f\"Datos etiquetados entrenamiento: {x_train.shape}\")\n",
    "print(f\"Etiquetas entrenamiento: {y_train.shape}\")\n",
    "print(f\"Datos validaci√≥n: {x_val.shape}\")\n",
    "print(f\"Etiquetas validaci√≥n: {y_val.shape}\")\n",
    "print(f\"Datos prueba: {x_test.shape}\")\n",
    "print(f\"Etiquetas prueba: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO 1\n",
    "\n",
    "Entrena un modelo, creado sobre TensorFlow, haciendo uso √∫nicamente de las instancias etiquetadas de entrenamiento. Dicho modelo debe de tener al menos cuatro capas densas y/o convolucionales.\n",
    "\n",
    "Responde a las siguientes preguntas:\n",
    "\n",
    "1. ¬øQu√© red has escogido? ¬øPor qu√©? ¬øC√≥mo la has entrenado?\n",
    "2. ¬øCu√°l es el rendimiento del modelo en entrenamiento? ¬øY en prueba?\n",
    "3. ¬øQu√© conclusiones sacas de los resultados detallados en el punto anterior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_base = ConvModel()\n",
    "history_base = model_base.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    batch_size=128,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval√∫a con el conjunto de prueba\n",
    "test_accuracy = model_base.score(x_test, y_test)\n",
    "print(f\"Accuracy en conjunto de prueba: {test_accuracy}\")\n",
    "\n",
    "model_base.plot(history_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO 2\n",
    "\n",
    "Entrena el mismo modelo, incorporando las instancias no etiquetadas de entrenamiento mediante la t√©cnica de auto-aprendizaje. Opcionalmente, se ponderar√° cada instancia de entrada en funci√≥n de su calidad (o certeza).\n",
    "\n",
    "Responde a las siguientes preguntas:\n",
    "1. ¬øQu√© par√°metros has definido para el entrenamiento?\n",
    "2. ¬øCu√°l es el rendimiento del modelo en entrenamiento? ¬øY en prueba?\n",
    "3. ¬øSe mejoran los resultados obtenidos en el Ejercicio 1?\n",
    "4. ¬øQu√© conclusiones sacas de los resultados detallados en los puntos anteriores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para crear modelos consistentes durante self-training\n",
    "def create_model():\n",
    "    return ConvModel(\n",
    "        learning_rate=0.0005,  # Learning rate reducido para fine-tuning\n",
    "        dropout_prob=0.25,     \n",
    "        l2_lambda=0.005        \n",
    "    )\n",
    "\n",
    "# Aplica self-training con datos no etiquetados\n",
    "final_model = ConvModel.self_training_v2(\n",
    "    model_func=create_model,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,  \n",
    "    unlabeled_data=unlabeled_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    thresh=0.8,             \n",
    "    train_epochs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Eval√∫a el modelo final\n",
    "final_accuracy = final_model.score(x_test, y_test)\n",
    "print(f\"Accuracy del modelo final con self-training: {final_accuracy}\")\n",
    "print(f\"Mejora respecto al modelo base: {final_accuracy - test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO 3\n",
    "\n",
    "Entrena un modelo de aprendizaje semisupervisado de tipo autoencoder en dos pasos (primero el autoencoder, despu√©s el clasificador). La arquitectura del encoder debe ser exactamente la misma que la definida en los Ejercicios 1 y 2, a excepci√≥n del √∫ltimo bloque de capas.\n",
    "\n",
    "Responde a las siguientes preguntas:\n",
    "1. ¬øCu√°l es la arquitectura del modelo? ¬øY sus hiperpar√°metros?\n",
    "2. ¬øCu√°l es el rendimiento del modelo en entrenamiento? ¬øY en prueba?\n",
    "3. ¬øSe mejoran los resultados obtenidos en los Ejercicios 1 y 2?\n",
    "4. ¬øQu√© conclusiones sacas de los resultados detallados en los puntos anteriores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = TwoStepAutoEncoder(\n",
    "                                input_shape=unlabeled_train[0].shape,\n",
    "                                learning_rate=0.006,\n",
    "                                l2_lambda=0.0005,\n",
    "                                dropout_prob=0.1)\n",
    "classifier = TwoStepClassifier(\n",
    "                              l2_lambda=0.0005,\n",
    "                              dropout_prob=0.05,\n",
    "                               learning_rate=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TwoStepTraining(autoencoder=autoencoder, \n",
    "                classifier=classifier, \n",
    "                x_train=x_train, \n",
    "                y_train=one_hot_train, \n",
    "                unlabeled_train=unlabeled_train, \n",
    "                validation_data=(x_val, one_hot_val),\n",
    "                batch_size_autoencoder=256,\n",
    "                epochs_autoencoder=1,\n",
    "                batch_size_classifier=4096,\n",
    "                epochs_classifier=405)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_plot(autoencoder, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.score(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO 4\n",
    "\n",
    "Entrena un modelo de aprendizaje semisupervisado de tipo autoencoder en un paso (autoencoder y clasificador al mismo tiempo). La arquitectura del autoencoder ser√° la misma que la definida en el Ejercicio 3, y la combinaci√≥n de encoder y clasificador ser√° igual a la arquitectura definida en el\n",
    "Ejercicio 1.\n",
    "\n",
    "Responde a las siguientes preguntas:\n",
    "1. ¬øCu√°l es la arquitectura del modelo? ¬øY sus hiperpar√°metros?\n",
    "2. ¬øCu√°l es el rendimiento del modelo en entrenamiento? ¬øY en prueba?\n",
    "3. ¬øSe mejoran los resultados obtenidos en los ejercicios anteriores?\n",
    "4. ¬øQu√© conclusiones sacas de los resultados detallados en los puntos anteriores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 10:57:36.985038: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-04-04 10:57:36.985115: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-04-04 10:57:36.985130: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-04-04 10:57:36.985177: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-04 10:57:36.985209: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "one_step_autoencoder = OneStepAutoencoder(input_shape=unlabeled_train[0].shape,\n",
    "                                learning_rate=0.0015,\n",
    "                                decoder_extra_loss_weight = 0.45,\n",
    "                                l2_lambda=0.00005,\n",
    "                                dropout_prob=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 10:57:41.745475: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 3s/step - classifier_accuracy: 0.0238 - classifier_loss: 0.9097 - decoder_loss: 0.0601 - loss: 0.6990\n"
     ]
    }
   ],
   "source": [
    "h = OneStepTraining(one_step_autoencoder, \n",
    "                    x_train=x_train, \n",
    "                    y_train=one_hot_train, \n",
    "                    unlabeled_train=unlabeled_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=1,\n",
    "                    patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 643ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reconstruction_plot(one_step_autoencoder, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_step_autoencoder.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO 5\n",
    "\n",
    "Repite el mismo entrenamiento de los Ejercicios 1-4, pero eliminando las instancias no etiquetadas\n",
    "m√°s at√≠picas con respecto a los datos etiquetados. Se cumplir√°n los siguientes puntos:\n",
    "- La arquitectura de la red de clasificaci√≥n en una clase ser√° la misma a la utilizada en el\n",
    "clasificador del Ejercicio 1, a excepci√≥n de la capa de salida.\n",
    "- Utiliza la t√©cnica explicada en el Notebook 5, usando un valor de ùë£ = 0,9.\n",
    "\n",
    "Responde a la siguiente pregunta:\n",
    "1. ¬øSe mejoran los resultados con respecto a los anteriores ejercicios? ¬øQu√© conclusiones sacas de estos resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pepe/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2025-04-03 23:44:05.002775: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-04-03 23:44:05.003053: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-04-03 23:44:05.003085: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-04-03 23:44:05.003349: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-03 23:44:05.003403: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/Users/pepe/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 23:44:06.559175: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m258/258\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step- loss: 0.3736 \n",
      "Cambiando r a 0.69512767 , max: 0.70241475 , min: 0.6930452\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 656ms/step - loss: 0.3712\n",
      "Epoch 2/2\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step- loss: 0.0720 \n",
      "Cambiando r a 0.6791968 , max: 0.69847983 , min: 0.67324144\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 618ms/step - loss: 0.0717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential, built=True>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalyDetector = AnomalyDetector(input_shape=(32,32,3), \n",
    "                        nu=.9,\n",
    "                        l2_lambda=0.0,\n",
    "                        learning_rate=0.0001,\n",
    "                        dropout_prob=0.0)\n",
    "anomalyDetector.fit(x_train, \n",
    "          batch_size=256, \n",
    "          epochs=2, \n",
    "          delta=.025, \n",
    "          steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_unlabeled_train, is_typical = anomaly_report(anomalyDetector, unlabeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo que va pero no estoy seguro\n",
    "#\n",
    "# plot_atipicos(filtered_unlabeled_train, is_typical, unlabeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 2 con datos filtrados\n",
    "model_self_filtered = ConvModel.self_training_v2(\n",
    "    model_func=create_model,\n",
    "    x_train=x_train_labeled,\n",
    "    y_train=y_train_labeled,\n",
    "    unlabeled_data=filtered_unlabeled_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    thresh=0.8,\n",
    "    train_epochs=1\n",
    ")\n",
    "\n",
    "# Evaluar los modelos filtrados\n",
    "test_accuracy_self_filtered = model_self_filtered.score(x_test, y_test)\n",
    "print(f\"Accuracy en conjunto de prueba (self-training filtrado): {test_accuracy_self_filtered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 3 con datos filtrados\n",
    "autoencoder_filtered = TwoStepAutoEncoder(\n",
    "    input_shape=filtered_unlabeled_train[0].shape,\n",
    "    learning_rate=0.006,\n",
    "    l2_lambda=0.0005,\n",
    "    dropout_prob=0.1\n",
    ")\n",
    "\n",
    "classifier_filtered = TwoStepClassifier(\n",
    "    l2_lambda=0.0005,\n",
    "    dropout_prob=0.05,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "\n",
    "history_two_step_filtered = TwoStepTraining(\n",
    "    autoencoder=autoencoder_filtered, \n",
    "    classifier=classifier_filtered, \n",
    "    x_train=x_train, \n",
    "    y_train=one_hot_train, \n",
    "    unlabeled_train=filtered_unlabeled_train, \n",
    "    validation_data=(x_val, one_hot_val),\n",
    "    batch_size_autoencoder=256,\n",
    "    epochs_autoencoder=50,\n",
    "    batch_size_classifier=256,\n",
    "    epochs_classifier=100\n",
    ")\n",
    "\n",
    "# Evaluar modelos\n",
    "test_accuracy_two_step_filtered = classifier_filtered.score(x_test, y_test)  # No se si es asi como se evaluaria este la verdad supongo que si\n",
    "print(f\"Accuracy del modelo one-step filtrado: {test_accuracy_two_step_filtered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 4 con datos filtrados\n",
    "one_step_autoencoder_filtered = OneStepAutoencoder(\n",
    "    input_shape=filtered_unlabeled_train[0].shape,\n",
    "    learning_rate=0.0015,\n",
    "    decoder_extra_loss_weight=0.45,\n",
    "    l2_lambda=0.00005,\n",
    "    dropout_prob=0.05\n",
    ")\n",
    "\n",
    "history_one_step_filtered = OneStepTraining(\n",
    "    one_step_autoencoder_filtered, \n",
    "    x_train=x_train, \n",
    "    y_train=one_hot_train, \n",
    "    unlabeled_train=filtered_unlabeled_train,\n",
    "    batch_size=256,\n",
    "    epochs=100,\n",
    "    patience=10\n",
    ")\n",
    "# Evaluar modelos\n",
    "test_accuracy_one_step_filtered = one_step_autoencoder_filtered.score(x_test, y_test)\n",
    "print(f\"Accuracy del modelo one-step filtrado: {test_accuracy_one_step_filtered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO 6\n",
    "\n",
    "Repite los Ejercicios 3-5 cambiando el autencoder por la t√©cnica definida en el apartado ‚ÄúHay vida m√°s all√° del autoencoder‚Äù del Notebook 4. Contesta a las preguntas de dichos ejercicios. Se cumplir√°n los siguientes puntos:\n",
    "\n",
    "1. La arquitectura de la red ser√° igual a la parte encoder del autencoder definido en los\n",
    "ejercicios anteriores.\n",
    "2. El modelo debe entrenar correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejercicio 3 two step con todos los datos\n",
    "\n",
    "cModel = ContrastiveModel(unlabeled_train[0].shape, \n",
    "                          learning_rate=0.05, \n",
    "                          lambda_param=.9,\n",
    "                          l2_lambda=0.001,\n",
    "                          dropout_prob=0.001)\n",
    "classifier = TwoStepClassifier(\n",
    "                              l2_lambda=0.0005,\n",
    "                              dropout_prob=0.05,\n",
    "                               learning_rate=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................................Epoch 1/20, Total Loss: 5.1303, Contrastive Loss: 4.9989, Clustering Loss: 0.1461\n",
      "..................................................................................................................................Epoch 2/20, Total Loss: 4.5734, Contrastive Loss: 4.5734, Clustering Loss: 0.0000\n",
      ".........................................."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/var/folders/1l/sdrz_tw104ld922_w0xnl5840000gn/T/ipykernel_2703/2410891534.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m TwoStepTraining(autoencoder=cModel, \n\u001b[32m      2\u001b[39m                 classifier=classifier,\n\u001b[32m      3\u001b[39m                 x_train=x_train,\n\u001b[32m      4\u001b[39m                 y_train=one_hot_train,\n",
      "\u001b[32m~/repositorios/3year2sem/maai/p2/AutoEncoder.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(autoencoder, classifier, x_train, y_train, unlabeled_train, validation_data, batch_size_autoencoder, epochs_autoencoder, batch_size_classifier, epochs_classifier, contrastive)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m TwoStepTraining(autoencoder, classifier, x_train, y_train, unlabeled_train, validation_data=\u001b[38;5;28;01mNone\u001b[39;00m, batch_size_autoencoder=\u001b[32m1024\u001b[39m, epochs_autoencoder=\u001b[32m100\u001b[39m, batch_size_classifier=\u001b[32m1024\u001b[39m, epochs_classifier=\u001b[32m100\u001b[39m, contrastive=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    158\u001b[39m \n\u001b[32m    159\u001b[39m     all_x = np.vstack((x_train, unlabeled_train))\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m contrastive:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m         autoencoder.train(unlabeled_train, epochs=epochs_autoencoder, batch_size= batch_size_autoencoder)\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    163\u001b[39m         autoencoder.fit(all_x, batch_size=batch_size_autoencoder, epochs=epochs_autoencoder, validation_data=(validation_data[\u001b[32m0\u001b[39m],validation_data[\u001b[32m0\u001b[39m]))\n\u001b[32m    164\u001b[39m     x_coded = autoencoder.get_encoded_data(x_train)\n",
      "\u001b[32m~/repositorios/3year2sem/maai/p2/Contrastive.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dataset, epochs, batch_size, temperature)\u001b[39m\n\u001b[32m    182\u001b[39m             batch_count = \u001b[32m0\u001b[39m\n\u001b[32m    183\u001b[39m \n\u001b[32m    184\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;28;01min\u001b[39;00m self.mini_batches(dataset, batch_size=batch_size):\n\u001b[32m    185\u001b[39m                 print(\u001b[33m'.'\u001b[39m,end=\u001b[33m''\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m                 loss_dict = self.train_step(data, temperature=temperature)\n\u001b[32m    187\u001b[39m                 epoch_total_loss += loss_dict[\u001b[33m\"loss\"\u001b[39m]\n\u001b[32m    188\u001b[39m                 epoch_contrastive_loss += loss_dict[\u001b[33m\"contrastive_loss\"\u001b[39m]\n\u001b[32m    189\u001b[39m                 epoch_clustering_loss += loss_dict[\u001b[33m\"clustering_loss\"\u001b[39m]\n",
      "\u001b[32m~/repositorios/3year2sem/maai/p2/Contrastive.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, data, temperature)\u001b[39m\n\u001b[32m    154\u001b[39m \n\u001b[32m    155\u001b[39m         \u001b[38;5;66;03m# Calcular gradientes y actualizar pesos\u001b[39;00m\n\u001b[32m    156\u001b[39m         gradients = tape.gradient(total_loss, self.encoder.trainable_variables + self.cluster.trainable_variables)\n\u001b[32m    157\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         grad_norm = tf.linalg.global_norm(gradients)\n\u001b[32m    159\u001b[39m \n\u001b[32m    160\u001b[39m         self.optimicer.apply_gradients(zip(gradients, self.encoder.trainable_variables + self.cluster.trainable_variables))\n\u001b[32m    161\u001b[39m \n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/util/dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/ops/clip_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(t_list, name)\u001b[39m\n\u001b[32m    284\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(v):\n\u001b[32m    285\u001b[39m           half_squared_norms.append(gen_nn_ops.l2_loss(v))\n\u001b[32m    286\u001b[39m \n\u001b[32m    287\u001b[39m     half_squared_norm = math_ops.reduce_sum(\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m         array_ops_stack.stack(half_squared_norms))\n\u001b[32m    289\u001b[39m \n\u001b[32m    290\u001b[39m     norm = math_ops.sqrt(\n\u001b[32m    291\u001b[39m         half_squared_norm *\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/util/dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/ops/array_ops_stack.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(values, axis, name)\u001b[39m\n\u001b[32m     71\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     73\u001b[39m       \u001b[38;5;66;03m# If the input is a constant list, it can be converted to a constant op\u001b[39;00m\n\u001b[32m     74\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m ops.convert_to_tensor(values, name=name)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError, NotImplementedError):\n\u001b[32m     76\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Input list contains non-constant tensors\u001b[39;00m\n\u001b[32m     77\u001b[39m \n\u001b[32m     78\u001b[39m   value_shape = ops.convert_to_tensor(values[\u001b[32m0\u001b[39m], name=name)._shape_tuple()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/profiler/trace.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m wrapped(*args, **kwargs):\n\u001b[32m    180\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m enabled:\n\u001b[32m    181\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, **trace_kwargs):\n\u001b[32m    182\u001b[39m           \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[39m\n\u001b[32m    709\u001b[39m ) -> Union[EagerTensor, SymbolicTensor]:\n\u001b[32m    710\u001b[39m   \u001b[33m\"\"\"Implementation of the public convert_to_tensor.\"\"\"\u001b[39m\n\u001b[32m    711\u001b[39m   \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[32m    712\u001b[39m   preferred_dtype = preferred_dtype \u001b[38;5;28;01mor\u001b[39;00m dtype_hint\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m   return tensor_conversion_registry.convert(\n\u001b[32m    714\u001b[39m       value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n\u001b[32m    715\u001b[39m   )\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    230\u001b[39m                   f\"actual = {ret.dtype.base_dtype.name}\",\n\u001b[32m    231\u001b[39m                   name=name))\n\u001b[32m    232\u001b[39m \n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n\u001b[32m    235\u001b[39m \n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;28;01mis\u001b[39;00m NotImplemented:\n\u001b[32m    237\u001b[39m       \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/ops/array_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(v, dtype, name, as_ref)\u001b[39m\n\u001b[32m   1301\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1302\u001b[39m     dtype = inferred_dtype\n\u001b[32m   1303\u001b[39m   \u001b[38;5;28;01melif\u001b[39;00m dtype != inferred_dtype:\n\u001b[32m   1304\u001b[39m     v = nest.map_structure(_cast_nested_seqs_to_dtype(dtype), v)\n\u001b[32m-> \u001b[39m\u001b[32m1305\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _autopacking_helper(v, dtype, name \u001b[38;5;28;01mor\u001b[39;00m \u001b[33m\"packed\"\u001b[39m)\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/ops/array_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(list_or_tuple, dtype, name)\u001b[39m\n\u001b[32m   1208\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m context.executing_eagerly():\n\u001b[32m   1209\u001b[39m     \u001b[38;5;66;03m# NOTE: Fast path when all the items are tensors, this doesn't do any type\u001b[39;00m\n\u001b[32m   1210\u001b[39m     \u001b[38;5;66;03m# checking.\u001b[39;00m\n\u001b[32m   1211\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m all(isinstance(elem, core.Tensor) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;28;01min\u001b[39;00m list_or_tuple):\n\u001b[32m-> \u001b[39m\u001b[32m1212\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m gen_array_ops.pack(list_or_tuple, name=name)\n\u001b[32m   1213\u001b[39m   must_pack = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1214\u001b[39m   converted_elems = []\n\u001b[32m   1215\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(name) \u001b[38;5;28;01mas\u001b[39;00m scope:\n",
      "\u001b[32m~/carrera/3/2/ma2/ma2Python12/lib/python3.12/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(values, axis, name)\u001b[39m\n\u001b[32m   6715\u001b[39m         _ctx, \u001b[33m\"Pack\"\u001b[39m, name, values, \u001b[33m\"axis\"\u001b[39m, axis)\n\u001b[32m   6716\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   6717\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   6718\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m-> \u001b[39m\u001b[32m6719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m   6720\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   6721\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   6722\u001b[39m       return pack_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "TwoStepTraining(autoencoder=cModel, \n",
    "                classifier=classifier, \n",
    "                x_train=x_train, \n",
    "                y_train=one_hot_train, \n",
    "                unlabeled_train=unlabeled_train, \n",
    "                validation_data=(x_val, one_hot_val),\n",
    "                batch_size_autoencoder=256,\n",
    "                epochs_autoencoder=20,\n",
    "                batch_size_classifier=4096,\n",
    "                epochs_classifier=20, \n",
    "                contrastive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 1/35, Total Loss: 8.1865, Contrastive Loss: 4.1832, Clustering Loss: 0.0279, Supervised Loss: 3.9782\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 2/35, Total Loss: 7.8341, Contrastive Loss: 3.8475, Clustering Loss: 0.0000, Supervised Loss: 3.9866\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 3/35, Total Loss: 7.8146, Contrastive Loss: 3.8226, Clustering Loss: 0.0000, Supervised Loss: 3.9920\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 4/35, Total Loss: 7.8030, Contrastive Loss: 3.8189, Clustering Loss: 0.0000, Supervised Loss: 3.9841\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 5/35, Total Loss: 7.8038, Contrastive Loss: 3.8162, Clustering Loss: 0.0000, Supervised Loss: 3.9877\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 6/35, Total Loss: 7.8018, Contrastive Loss: 3.8174, Clustering Loss: 0.0000, Supervised Loss: 3.9844\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 7/35, Total Loss: 7.8015, Contrastive Loss: 3.8188, Clustering Loss: 0.0000, Supervised Loss: 3.9826\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 8/35, Total Loss: 7.8045, Contrastive Loss: 3.8190, Clustering Loss: 0.0000, Supervised Loss: 3.9855\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 9/35, Total Loss: 7.8076, Contrastive Loss: 3.8218, Clustering Loss: 0.0000, Supervised Loss: 3.9859\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 10/35, Total Loss: 7.8073, Contrastive Loss: 3.8200, Clustering Loss: 0.0000, Supervised Loss: 3.9873\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 11/35, Total Loss: 7.8066, Contrastive Loss: 3.8193, Clustering Loss: 0.0000, Supervised Loss: 3.9873\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 12/35, Total Loss: 7.8082, Contrastive Loss: 3.8202, Clustering Loss: 0.0000, Supervised Loss: 3.9881\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 13/35, Total Loss: 7.8053, Contrastive Loss: 3.8187, Clustering Loss: 0.0000, Supervised Loss: 3.9866\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 14/35, Total Loss: 7.8038, Contrastive Loss: 3.8190, Clustering Loss: 0.0000, Supervised Loss: 3.9848\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 15/35, Total Loss: 7.8051, Contrastive Loss: 3.8181, Clustering Loss: 0.0000, Supervised Loss: 3.9870\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 16/35, Total Loss: 7.8061, Contrastive Loss: 3.8188, Clustering Loss: 0.0000, Supervised Loss: 3.9873\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 17/35, Total Loss: 7.8066, Contrastive Loss: 3.8207, Clustering Loss: 0.0000, Supervised Loss: 3.9859\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 18/35, Total Loss: 7.8054, Contrastive Loss: 3.8188, Clustering Loss: 0.0000, Supervised Loss: 3.9866\n",
      "............................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 19/35, Total Loss: 7.8035, Contrastive Loss: 3.8191, Clustering Loss: 0.0000, Supervised Loss: 3.9844\n",
      ".........................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "# ejercicio 4 ONE STEP con todos los datos\n",
    "\n",
    "cSSLModel = SemiSupervisedContrastiveModel(unlabeled_train[0].shape, \n",
    "                          learning_rate=0.1, \n",
    "                          lambda_param=.9,\n",
    "                          l2_lambda=0.0001,\n",
    "                          dropout_prob=0.001,\n",
    "                          lambda_supervised=1.0)\n",
    "\n",
    "\n",
    "cSSLModel.train(\n",
    "    X_unlabeled=unlabeled_train,  \n",
    "    X_labeled=x_train,        \n",
    "    y_labeled=y_train,       \n",
    "    epochs=35,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "cSSLModel.plot_training_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "test_samples = x_test[:n_samples].reshape(n_samples,32,32,3)\n",
    "\n",
    "cSSLModel.plot_similarity_matrix(test_samples, n_samples=n_samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejercicio 3 two step con los datos filtrados\n",
    "\n",
    "cModel = ContrastiveModel(unlabeled_train[0].shape, \n",
    "                          learning_rate=0.05, \n",
    "                          lambda_param=.9,\n",
    "                          l2_lambda=0.001,\n",
    "                          dropout_prob=0.001)\n",
    "classifier = TwoStepClassifier(\n",
    "                              l2_lambda=0.0005,\n",
    "                              dropout_prob=0.05,\n",
    "                               learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoStepTraining(autoencoder=cModel, \n",
    "                classifier=classifier, \n",
    "                x_train=x_train, \n",
    "                y_train=one_hot_train, \n",
    "                unlabeled_train=filtered_unlabeled_train, # <-\n",
    "                validation_data=(x_val, one_hot_val),\n",
    "                batch_size_autoencoder=256,\n",
    "                epochs_autoencoder=1,\n",
    "                batch_size_classifier=4096,\n",
    "                epochs_classifier=1, \n",
    "                contrastive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejercicio 4 ONE STEP con los datos filtrados\n",
    "\n",
    "cSSLModel = SemiSupervisedContrastiveModel(unlabeled_train[0].shape, \n",
    "                          learning_rate=0.05, \n",
    "                          lambda_param=.9,\n",
    "                          l2_lambda=0.001,\n",
    "                          dropout_prob=0.001,\n",
    "                          lambda_supervised=1.0)\n",
    "\n",
    "\n",
    "cSSLModel.train(\n",
    "    X_unlabeled=filtered_unlabeled_train,  # <-\n",
    "    X_labeled=x_train,        \n",
    "    y_labeled=y_train,       \n",
    "    epochs=6,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "cSSLModel.plot_training_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "test_samples = x_test[:n_samples].reshape(n_samples,32,32,3)\n",
    "\n",
    "cSSLModel.plot_similarity_matrix(test_samples, n_samples=n_samples);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma2Python12",
   "language": "python",
   "name": "ma2python12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
