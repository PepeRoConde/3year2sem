{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57knM8jrYZ2t"
   },
   "source": [
    "# Notebook 3: Autoaprendizaje\n",
    "\n",
    "## Pre-requisitos\n",
    "\n",
    "### Instalar paquetes\n",
    "\n",
    "Si la práctica requiere algún paquete de Python, habrá que incluir una celda en la que se instalen. Si usamos un paquete que se ha utilizado en prácticas anteriores, podríamos dar por supuesto que está instalado pero no cuesta nada satisfacer todas las dependencias en la propia práctica para reducir las dependencias entre ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LkaimNJfYZ2w"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de instalación de tensorflow 2.0\n",
    "#%tensorflow_version 2.x\n",
    "# !pip3 install tensorflow  # NECESARIO SOLO SI SE EJECUTA EN LOCAL\n",
    "import tensorflow as tf\n",
    "\n",
    "# Hacemos los imports que sean necesarios\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOch-CnwQttl"
   },
   "source": [
    "# Autoaprendizaje sobre Fashion-MNIST\n",
    "\n",
    "Lo primero que tenemos que hacer es cargar el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1gXdWDBIEKel"
   },
   "outputs": [],
   "source": [
    "labeled_data = 0.05 # Vamos a usar el etiquetado de sólo el 5% de los datos\n",
    "np.random.seed(42)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "indexes = np.arange(len(x_train))\n",
    "np.random.shuffle(indexes)\n",
    "ntrain_data = int(labeled_data*len(x_train))\n",
    "unlabeled_train = x_train[indexes[ntrain_data:]]\n",
    "x_train = x_train[indexes[:ntrain_data]]\n",
    "y_train = y_train[indexes[:ntrain_data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8XsZIqV8TmSc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 784) (10000, 784) (57000, 784)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Haz el preprocesado que necesites aquí (si lo necesitas)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1] * x_test.shape[2]))\n",
    "unlabeled_train = np.reshape(unlabeled_train, (unlabeled_train.shape[0], unlabeled_train.shape[1] * unlabeled_train.shape[2]))\n",
    "print(x_train.shape, x_test.shape, unlabeled_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkaCDOGapMyl"
   },
   "source": [
    "## Función de autoaprendizaje\n",
    "\n",
    "Vamos a crear nuestra propia función de autoaprendizaje. Para ello, vamos a utilizar el siguiente pseudocódigo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qBChtzdTZU9"
   },
   "source": [
    "\n",
    "\n",
    "**self_training** *(model, x_train, y_train, unlabeled_data, thresh, train_epochs)*\n",
    "\n",
    "1. $train\\_data, train\\_label \\leftarrow x\\_train, y\\_train$\n",
    "1. **Desde** $n = 1 .. train\\_epochs$ **hacer**\n",
    "\t1. Entrena el *model* usando las variables *train\\_data* y *train\\_label* \n",
    "\t2. $y\\_pred \\leftarrow model(unlabeled\\_data)$\n",
    "  \n",
    "  3. $y\\_class, y\\_value \\leftarrow $ Clase ganadora en *y_pred* con su valor\n",
    "\n",
    "  4. $train\\_data, train\\_label \\leftarrow x\\_train, y\\_train$\n",
    "  \n",
    "  5. **Para cada elemento** (x_u, y_c, y_v) **de la tupla** (unlabeled_data, y_class, y_value) \n",
    "\t  1. **Si** $y\\_v > thresh$ **entonces**\n",
    "\t\t\n",
    "        1. Añadimos $x\\_u$ e $y\\_c$ a train\\_data y train\\_label, respectivamente.\n",
    "\n",
    "4. Devolvemos el modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqT2nuCspfE_"
   },
   "source": [
    "<font color='red'>NOTA:</font> para entrenar (y predecir) vamos a utilizar los modelos de Sklearn. Familiarízate con las funciones [fit](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.fit), [predict](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict), [predict_proba](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict_proba) y [score](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5xjcLa21EKen"
   },
   "outputs": [],
   "source": [
    "# TODO: implementa el algoritmo self_training tal y como viene en el pseudocódigo. Las variables extra son para epara visualización de resultados\n",
    "\n",
    "def self_training(model_func, x_train, y_train, unlabeled_data, x_test, y_test, thresh=0.8, train_epochs=3):\n",
    "    train_data = x_train.copy()\n",
    "    train_label = y_train.copy()\n",
    "\n",
    "    for i in range(train_epochs):\n",
    "        model = model_func()\n",
    "        model.fit(train_data, train_label)\n",
    "        # Predecir en los datos no etiquetados\n",
    "        # model.predict_proba() devuelve la probabilidad de cada clase para cada ejemplo \n",
    "        # (n_samples, n_classes)\n",
    "        y_pred = model.predict_proba(unlabeled_data)\n",
    "        # Necesitamos usar axis=1 para obtener el máximo de cada fila, es decir, de cada ejemplo\n",
    "        # de esta forma obtenemos el valor de la clase predicha y la probabilidad de la clase predicha\n",
    "        y_class, y_value = np.argmax(y_pred, axis=1), np.max(y_pred, axis=1)\n",
    "        # Reseteamos los datos de entrenamiento\n",
    "        train_data = x_train.copy()\n",
    "        train_label = y_train.copy()\n",
    "        \n",
    "        # Añadimos los datos con probabilidad mayor que el thresh\n",
    "        for i in range(len(unlabeled_data)):\n",
    "            if y_value[i] > thresh:\n",
    "                train_data = np.append(train_data, [unlabeled_data[i]], axis=0)\n",
    "                train_label = np.append(train_label, [y_class[i]], axis=0)\n",
    "        print(f\"Epoch {i+1} - Added {len(train_data) - len(x_train)} samples\")\n",
    "    model = model_func()\n",
    "    model.fit(train_data, train_label)\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjFXe6EiYfRg"
   },
   "source": [
    "### Entrenamos nuestro clasificador\n",
    "\n",
    "Usa lo hecho anteriormente para entrenar tu clasificador de una manera semi-supervisada. Utiliza para ello el [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) de sklearn (vigila el parámetro probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNC1s2Wmqx4x"
   },
   "outputs": [],
   "source": [
    "# Define la función para llamar al SVM\n",
    "from sklearn.svm import SVC\n",
    "model_func = lambda: SVC(kernel='linear', probability=True, max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hN2zd3DEYnKI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/circus/repos/3year2sem/maai/maai_env/lib/python3.11/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/circus/repos/3year2sem/maai/maai_env/lib/python3.11/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/circus/repos/3year2sem/maai/maai_env/lib/python3.11/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/circus/repos/3year2sem/maai/maai_env/lib/python3.11/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score:  0.7973\n"
     ]
    }
   ],
   "source": [
    "# TODO: Entrena tu clasificador\n",
    "model = self_training(model_func, x_train, y_train, unlabeled_train, x_test, y_test) \n",
    "print(\"Test Score: \", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbUKp14pPsrp"
   },
   "source": [
    "## Mejorando el código\n",
    "\n",
    "Tal como hemos visto en clase de teoría, este código puede ser mejorado de varias maneras:\n",
    "\n",
    "  1. **Asignar más peso a las variables etiquetadas**.\n",
    "  1. **Asignar un peso en función de la certeza de la predicción en las variables sin etiquetar**.\n",
    "\n",
    "### TRABAJO: Modifica la función self_training para tener en cuenta todos los puntos mencionados anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eF_9LMeZ2J2"
   },
   "outputs": [],
   "source": [
    "def self_training_v2(model_func, x_train, y_train, unlabeled_data, x_test, y_test, thresh=0.8, train_epochs=3):\n",
    "    train_data = x_train.copy()\n",
    "    train_label = y_train.copy()\n",
    "    # Inicializamos los pesos de las muestras a 2.0\n",
    "    sample_weights = np.ones(len(train_label)) * 2.0\n",
    "    \n",
    "    # Trabajamos directamente con unlabeled_data\n",
    "    current_unlabeled = unlabeled_data.copy()\n",
    "    \n",
    "    for i in range(train_epochs):\n",
    "        if len(current_unlabeled) == 0:\n",
    "            print(\"No more unlabeled data left\")\n",
    "            break\n",
    "            \n",
    "        model = model_func()\n",
    "        # Usamos los pesos de las muestras\n",
    "        model.fit(train_data, train_label, sample_weight=sample_weights)\n",
    "        \n",
    "        # Predicción en datos sin etiquetar\n",
    "        y_pred = model.predict_proba(current_unlabeled)\n",
    "        y_class = np.argmax(y_pred, axis=1)  # Clase con mayor probabilidad\n",
    "        y_value = np.max(y_pred, axis=1)     # Valor de probabilidad más alto\n",
    "        \n",
    "        # Seleccionar ejemplos con confianza superior al umbral\n",
    "        high_confidence = y_value > thresh\n",
    "        \n",
    "        if np.any(high_confidence):\n",
    "            # Obtener datos, etiquetas y probabilidades de los ejemplos de alta confianza\n",
    "            new_data = current_unlabeled[high_confidence]\n",
    "            new_labels = y_class[high_confidence]\n",
    "            new_probs = y_value[high_confidence]\n",
    "            \n",
    "            # Añadir a datos de entrenamiento\n",
    "            train_data = np.vstack([train_data, new_data])\n",
    "            train_label = np.append(train_label, new_labels)\n",
    "            sample_weights = np.append(sample_weights, new_probs)\n",
    "            \n",
    "            # Eliminar ejemplos usados\n",
    "            current_unlabeled = current_unlabeled[high_confidence == False]\n",
    "            \n",
    "            print(f\"Epoch {i+1}: {len(new_data)} samples added, {len(current_unlabeled)} remaining unlabeled\")\n",
    "        else:\n",
    "            print(f\"Epoch {i+1}: No samples added\")\n",
    "    \n",
    "    # Entrenamos el modelo final\n",
    "    model = model_func()\n",
    "    model.fit(train_data, train_label, sample_weight=sample_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YbqC0inexwHp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/circus/repos/3year2sem/maai/maai_env/lib/python3.11/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 27477 samples added, 29523 remaining unlabeled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/circus/repos/3year2sem/maai/maai_env/lib/python3.11/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 8083 samples added, 21440 remaining unlabeled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/circus/repos/3year2sem/maai/maai_env/lib/python3.11/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 5634 samples added, 15806 remaining unlabeled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/circus/repos/3year2sem/maai/maai_env/lib/python3.11/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score:  0.7852\n"
     ]
    }
   ],
   "source": [
    "# TODO: Entrena tu clasificador\n",
    "model_func = lambda: SVC(kernel='linear', probability=True, max_iter=200)\n",
    "model = self_training_v2(model_func, x_train, y_train, unlabeled_train, x_test, y_test) \n",
    "print(\"Test Score: \", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_knU62FdYJKn"
   },
   "source": [
    "### Creamos nuestro clasificador en Tensorflow\n",
    "\n",
    "Ya deberías de ser capaz de realizar este trabajo con muy poca ayuda. El diseño del clasificador es libre (capas densas, convolucionales, ...), puedes crearlo como quieras. Lo único que tenemos que tener en cuenta es que tenemos que encapsular nuestro modelo en una clase cuyas funciones tengan el mismo nombre (y los mismos parámetros) que las existentes en sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "dG31TKIraE7r"
   },
   "outputs": [],
   "source": [
    "# TODO: crea tu propio clasificador\n",
    "\n",
    "class MiClasificador:\n",
    "\n",
    "    def __init__(self):\n",
    "        # TODO : define el modelo\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        # TODO: crea el optimizador\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        # TODO: compila el modelo\n",
    "        self.model.compile(\n",
    "            loss=self.loss,\n",
    "            optimizer=self.optimizer,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # TODO: entrena el modelo. Escoge el tamaño de batch y el número de epochs que quieras\n",
    "        self.model.fit(X, y, sample_weight=sample_weight, batch_size=64, epochs=5, verbose=0)\n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO: devuelve la clase ganadora\n",
    "        return np.argmax(self.model.predict(X), axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # TODO: devuelve las probabilidades de cada clase\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # TODO: devuelve el accuracy del clasificador\n",
    "        _, acc = self.model.evaluate(X, y)\n",
    "        return acc\n",
    "\n",
    "    def __del__(self):\n",
    "        del self.model\n",
    "        tf.keras.backend.clear_session() # Necesario para liberar la memoria en GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBdkgibORzyh"
   },
   "source": [
    "### Entrenando el modelo\n",
    "\n",
    "Crea una función que nos permita crear el modelo en cada iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "--x-RSJbeUdB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/circus/repos/3year2sem/maai/maai_env/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - accuracy: 0.7226 - loss: 2.8964\n",
      "Baseline Test Score:  0.7211999893188477\n"
     ]
    }
   ],
   "source": [
    "# TODO: Entrena el modelo\n",
    "def model_func():\n",
    "    return MiClasificador()\n",
    "\n",
    "baseline_model = model_func()\n",
    "baseline_model.fit(x_train, y_train)\n",
    "print(\"Baseline Test Score: \", baseline_model.score(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7_autoaprendizaje.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "maai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
