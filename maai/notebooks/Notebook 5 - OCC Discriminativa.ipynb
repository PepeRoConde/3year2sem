{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57knM8jrYZ2t"
   },
   "source": [
    "# Notebook 5: One Class Network\n",
    "\n",
    "## Pre-requisitos\n",
    "\n",
    "### Instalar paquetes\n",
    "\n",
    "Si la práctica requiere algún paquete de Python, habrá que incluir una celda en la que se instalen. Si usamos un paquete que se ha utilizado en prácticas anteriores, podríamos dar por supuesto que está instalado pero no cuesta nada satisfacer todas las dependencias en la propia práctica para reducir las dependencias entre ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "LkaimNJfYZ2w"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de instalación de tensorflow 2.0\n",
    "#%tensorflow_version 2.x\n",
    "# !pip3 install tensorflow  # NECESARIO SOLO SI SE EJECUTA EN LOCAL\n",
    "import tensorflow as tf\n",
    "\n",
    "# Hacemos los imports que sean necesarios\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOch-CnwQttl"
   },
   "source": [
    "# One Class sobre datos artificiales\n",
    "\n",
    "Lo primero que tenemos que hacer es definir los datos a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "1gXdWDBIEKel"
   },
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "rng = np.random.RandomState(random_state)\n",
    "#  datos de entrenamiento\n",
    "X = 0.3 * rng.randn(5000, 2)\n",
    "x_train = np.r_[X + 2, X - 2]\n",
    "#  datos de test en la misma distribución que los datos de entrenamiento\n",
    "X = 0.3 * rng.randn(200, 2)\n",
    "x_test = np.r_[X + 2, X - 2]\n",
    "#  outliers\n",
    "x_outliers = rng.uniform(low=-4, high=4, size=(200, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2) (400, 2) (200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape, x_outliers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkaCDOGapMyl"
   },
   "source": [
    "## Crea tu propia red para la detección de anomalías\n",
    "\n",
    "Vamos a crear nuestra propia red para la detección de anomalías. Para ello, vamos a definir una red cualquiera, que nos **transforme los datos de entrada en una salida de un único elemento**. Esta red va a cumplir una serie de características:\n",
    "\n",
    "* La capa anterior a la salida serán las llamadas **deep features**.\n",
    "* Todas las capas (incluyendo la última) deben incluir regularización.\n",
    "* La función de coste es $$L(y, \\tilde{y}) = \\dfrac{1}{2} \\| w^2 \\| + \\dfrac{1}{\\nu} \\dfrac{1}{N} \\sum_{i=1}^N \\max(0, r - \\tilde{y}) $$ donde $\\tilde{y}$ es la salida de la red, $\\nu$ es un hiperparámetro entre 0 y 1, y $r$ es un parámetro no entrenable, pero que va a ser modificado en cada epoch.\n",
    "* Al final del cada epoch, r va a ser modificado al valor del $\\nu$-cuantil de los datos de entrada (este valor será modificado gracias al Callback proporcionado a continuación).\n",
    "* Para la predicción, se considerará un dato típico si $\\tilde{y} > r$. En caso contrario, será un dato atípico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_JM1GSLe8nXc"
   },
   "outputs": [],
   "source": [
    "class ChangeRCallback(tf.keras.callbacks.Callback):\n",
    "   def __init__(self, train_data, delta=.025, steps=3):\n",
    "       super().__init__()\n",
    "       self.train_data = train_data\n",
    "       self.delta = delta\n",
    "       self.steps = steps\n",
    "       self.cont = 0\n",
    "\n",
    "   def on_epoch_end(self, epoch, logs=None):\n",
    "       sorted_values = np.sort(self.model.predict(self.train_data).flatten())\n",
    "       new_value = sorted_values[int(len(sorted_values) * (1. - self.model.nu))]\n",
    "       old_value = self.model.r.numpy()\n",
    "       print('Cambiando r a', new_value, ', max:', sorted_values.max(), ', min:', sorted_values.min())\n",
    "       self.model.r.assign(new_value)\n",
    "       if np.abs(old_value - new_value) < self.delta:\n",
    "            self.cont += 1\n",
    "            if self.cont >= self.steps:\n",
    "                print('Convergencia obtenida. Finalizando el entrenamiento.')\n",
    "                self.model.stop_training = True\n",
    "       else:\n",
    "            self.cont = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nroMok1bG9p0"
   },
   "source": [
    "Tu trabajo es crear el modelo y entrenarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5xjcLa21EKen"
   },
   "outputs": [],
   "source": [
    "# TODO: implementa la red de detección de anomalías\n",
    "\n",
    "class DetectorAnomalias:\n",
    "\n",
    "\tdef __init__(self, input_shape, nu=.5):\n",
    "\t\t# TODO : define el modelo\n",
    "\n",
    "\t\tself.model = tf.keras.models.Sequential([\n",
    "\t\t\ttf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "\t\t\t# Primera capa densa con regularización L2\n",
    "\t\t\ttf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "\t\t\t# Segunda capa densa (deep features) con regularización L2\n",
    "\t\t\ttf.keras.layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), name='deep_features'),\n",
    "\t\t\t# Capa de salida con regularización L2 (un único valor escalar)\n",
    "\t\t\ttf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\t\t])\n",
    "\n",
    "\t\tself.model.r = tf.Variable(1.0, trainable=False, name='r', dtype=tf.float32)\n",
    "\t\tself.model.nu = tf.Variable(nu, trainable=False, name='nu', dtype=tf.float32)\n",
    "\t\t\n",
    "\t  \n",
    "\tdef loss_function(self, y_true, y_pred):\n",
    "\t\t# TODO: crea la función de pérdida\n",
    "\t\t# w = self.model.layers[-1].kernel\n",
    "\t\tr = self.model.r\n",
    "\t\tnu = self.model.nu\n",
    "\t\t\n",
    "\t\t# Primera parte: regularización L2 (ya incluida por los regularizadores de capa)\n",
    "\t\t# Segunda parte: término de error basado en el margen r\n",
    "\t\t# max(0, r - y_pred) para cada predicción\n",
    "\t\tmargin_error = tf.maximum(0.0, r - y_pred)\n",
    "\t\t# Calculamos la media y aplicamos el factor 1/nu\n",
    "\t\tmargin_loss = tf.reduce_mean(margin_error) / nu\n",
    "\t\t\n",
    "\t\t# La función de pérdida total (la regularización ya está incluida en las capas)\n",
    "\t\treturn margin_loss\n",
    "\t\n",
    "\tdef fit(self, X, y=None, sample_weight=None):\n",
    "\t\t# TODO: entrena el modelo. Escoge el tamaño de batch y el número de epochs que quieras. No te olvides del callback.\n",
    "\t\tdummy_y = np.zeros((len(X), 1)) # Necesario pasar como salida para que keras no de un error\n",
    "\n",
    "\t\tself.model.compile(optimizer='adam', loss=self.loss_function)\n",
    "\t\tself.model.fit(X, dummy_y, callbacks=[ChangeRCallback(X)], batch_size=64, epochs=5, verbose=1)\n",
    "\t\treturn self.model\n",
    "\t\t\n",
    "\tdef predict(self, X):\n",
    "\t\t# TODO: Devuelve la predicción del modelo\n",
    "\t\treturn self.model.predict(X)\n",
    "\n",
    "\tdef get_encoded_data(self, X):\n",
    "\t\t# TODO: devuelve la salida del encoder (code)\n",
    "\t\treturn self.model.layers[1].predict(X)\n",
    "\t\t\n",
    "\tdef __del__(self):\n",
    "\t\t# TODO: borra el modelo\n",
    "\t\tdel self.model\n",
    "\t\ttf.keras.backend.clear_session() # Necesario para liberar la memoria en GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjFXe6EiYfRg"
   },
   "source": [
    "### Entrena el modelo.\n",
    "\n",
    "Usa lo hecho anteriormente para entrenar tu modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "lNC1s2Wmqx4x"
   },
   "outputs": [],
   "source": [
    "# TODO: Define el modelo\n",
    "def model_func():\n",
    "    model = DetectorAnomalias(input_shape=(2,))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "hN2zd3DEYnKI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clown/2-semester/3year2sem/maai/maai_env/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step loss:\n",
      "Cambiando r a 1.3756768 , max: 1.8922739 , min: 0.85561347\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.9266\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss:\n",
      "Cambiando r a 1.7590346 , max: 2.345709 , min: 1.1460744\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1974  \n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss:\n",
      "Cambiando r a 2.339187 , max: 3.0768006 , min: 1.5794132\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1615  \n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss:\n",
      "Cambiando r a 3.036864 , max: 4.109327 , min: 2.0036867\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1426  \n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss:\n",
      "Cambiando r a 3.9693103 , max: 5.157235 , min: 2.7319875\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1365  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_3, built=True>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Entrena tu modelo\n",
    "model = model_func()\n",
    "model.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbUKp14pPsrp"
   },
   "source": [
    "## Evaluando el modelo\n",
    "\n",
    "Una vez entrenado, para evaluar el modelo sólo hay que tener en cuenta lo siguiente:\n",
    "\n",
    "  1. Si la salida es mayor que r, es un dato típico.\n",
    "  1. Si la salida es menor que r, es un dato atípico.\n",
    "\n",
    "### TRABAJO: Evalúa el modelo con los datos del conjunto de test, y con los outliers. Visualiza los datos típicos y atípicos con una gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "7eF_9LMeZ2J2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Valor de r: 3.9693\n",
      "Porcentaje de datos etiquetados como típicos: 49.50%\n",
      "Porcentaje de datos etiquetados como anomalías: 50.50%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evalúa el modelo con los datos del conjunto de test. Indica el porcentaje de datos etiquetados como típicos, y visualiza los datos\n",
    "predicciones = model.predict(x_test)\n",
    "\n",
    "# Determinamos qué datos son típicos (normales) y cuáles son anomalías\n",
    "# Un dato es típico si su predicción es mayor que r\n",
    "r_valor = model.model.r.numpy()\n",
    "es_tipico = predicciones > r_valor\n",
    "\n",
    "# Calculamos el porcentaje de datos etiquetados como típicos\n",
    "porcentaje_tipicos = np.mean(es_tipico) * 100\n",
    "\n",
    "print(f\"Valor de r: {r_valor:.4f}\")\n",
    "print(f\"Porcentaje de datos etiquetados como típicos: {porcentaje_tipicos:.2f}%\")\n",
    "print(f\"Porcentaje de datos etiquetados como anomalías: {100 - porcentaje_tipicos:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "YbqC0inexwHp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Porcentaje de outliers etiquetados como atípicos: 76.50%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evalúa el modelo con los datos del conjunto de outliers. Indica el porcentaje de datos etiquetados como atípicos, y visualiza los datos en conjunto con los de test\n",
    "predicciones_outliers = model.predict(x_outliers)\n",
    "r_valor = model.model.r.numpy()\n",
    "es_atipico_outliers = predicciones_outliers <= r_valor\n",
    "porcentaje_atipicos_outliers = np.mean(es_atipico_outliers) * 100\n",
    "\n",
    "# Evaluamos también los datos de test para comparar\n",
    "predicciones_test = model.predict(x_test)\n",
    "es_atipico_test = predicciones_test <= r_valor\n",
    "\n",
    "print(f\"Porcentaje de outliers etiquetados como atípicos: {porcentaje_atipicos_outliers:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDWehV02UIpl"
   },
   "source": [
    "¿Qué resultados has obtenido? Si el número de outliers detectado es bajo (inferior al 30%), puedes estar cometiendo algún error, entre ellos:\n",
    "\n",
    "* Sobreentrenar el modelo. Prueba a usar un delta distinto en el callback.\n",
    "* Usar un valor de $\\nu$ demasiado alto.\n",
    "\n",
    "Prueba distintas configuraciones para ver su efecto."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "9_oneclass.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "maai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
