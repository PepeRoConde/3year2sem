


from utils.mnist_reader import load_mnist
#import p1utils
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from  sklearn.metrics import adjusted_rand_score
from sklearn.cluster import KMeans, DBSCAN, HDBSCAN, AgglomerativeClustering, MeanShift, OPTICS, estimate_bandwidth, AffinityPropagation, SpectralClustering
from sklearn.decomposition import PCA
import sklearn.manifold
from  sklearn.model_selection import GridSearchCV
import numpy as np
from time import time
import umap
import torch
from torch import nn, tensor, optim
from torch.utils.data import DataLoader
import pandas as pd





X_train, y_train = load_mnist('data/fashion', kind='train')
X_test, y_test = load_mnist('data/fashion', kind='t10k')
X_train, X_dev = X_train[:50000], X_train[50000:]
y_train, y_dev = y_train[:50000], y_train[50000:]

print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')
print(f'X_dev: {X_dev.shape}, y_dev: {y_dev.shape}')
print(f'X_test: {X_test.shape}, y_test: {y_test.shape}')





indexes = np.random.choice(len(X_train),20)
plt.figure(figsize=(10, 8))
for i, index in enumerate(indexes):
    plt.subplot(4,5,i+1)
    plt.title(f'Class label: {y_train[index]}')
    plt.imshow(np.resize(X_train[index],(28,28)))
    
plt.show()









counts, _ = np.histogram(y_train)
plt.bar(range(len(counts)), counts/max(counts) ,color='cyan')
plt.ylim((0.9,1))
plt.title(f'cantidad de apararicion relativa a la clase más presente (#instancias {max(counts)})');





reducer = umap.UMAP()
X_train_reduced = reducer.fit_transform(X_train)
hdbscan1000 = HDBSCAN(min_cluster_size = 1000)
hdbscan100 = HDBSCAN(min_cluster_size = 100)
hdbscan10 = HDBSCAN(min_cluster_size = 10)
labelsHDBSCAN1000 = hdbscan1000.fit_predict(X_train_reduced)
labelsHDBSCAN100 = hdbscan100.fit_predict(X_train_reduced)
labelsHDBSCAN10 = hdbscan10.fit_predict(X_train_reduced)





plt.figure(figsize=(15, 5))
plt.subplot(1,4,1)
plt.title('Etiquetas originales')
plt.scatter(X_train_reduced[:,0],X_train_reduced[:,1],alpha=1e-2,c=y_train)
plt.subplot(1,4,2)
plt.title('Etiquetas HDBSCAN m.c.s 1000')
plt.scatter(X_train_reduced[:,0],X_train_reduced[:,1],alpha=1e-2,c=labelsHDBSCAN1000)
plt.subplot(1,4,3)
plt.title('Etiquetas HDBSCAN m.c.s 100')
plt.scatter(X_train_reduced[:,0],X_train_reduced[:,1],alpha=1e-2,c=labelsHDBSCAN100)
plt.subplot(1,4,4)
plt.title('Etiquetas HDBSCAN m.c.s 10')
plt.scatter(X_train_reduced[:,0],X_train_reduced[:,1],alpha=1e-2,c=labelsHDBSCAN10);





anomalies = np.where(labelsHDBSCAN100 == -1)[0]
indexes = np.random.choice(anomalies,20)
plt.figure(figsize=(10, 8))
for i, index in enumerate(indexes):
    plt.subplot(4,5,i+1)
    #plt.title(f'Class label: {y_train[index]}')
    plt.imshow(np.resize(X_train[index],(28,28)))
plt.show()











corners = np.array([
      
    [10, 7], 
    [15, 5],  
    [7, -2.5],
    [12, -5] 
    
])

test_pts = np.array([
    (corners[0]*(1-x) + corners[1]*x)*(1-y) +
    (corners[2]*(1-x) + corners[3]*x)*y
    for y in np.linspace(0, 1, 10)
    for x in np.linspace(0, 1, 10)
])

inv_transformed_points = reducer.inverse_transform(test_pts)


# código fusilado de https://umap-learn.readthedocs.io/en/latest/inverse_transform.html

fig = plt.figure(figsize=(12,6))
gs = GridSpec(10, 20, fig)
scatter_ax = fig.add_subplot(gs[:, :10])
digit_axes = np.zeros((10, 10), dtype=object)
for i in range(10):
    for j in range(10):
        digit_axes[i, j] = fig.add_subplot(gs[i, 10 + j])


scatter_ax.scatter(reducer.embedding_[:, 0], reducer.embedding_[:, 1],
                   c=y_train.astype(np.int32),  s=0.1)
scatter_ax.set(xticks=[], yticks=[])

scatter_ax.scatter(test_pts[:, 0], test_pts[:, 1], marker='x', c='k', s=15)

for i in range(10):
    for j in range(10):
        digit_axes[i, j].imshow(inv_transformed_points[i*10 + j].reshape(28, 28))
        digit_axes[i, j].set(xticks=[], yticks=[])





X_train_5D = umap.UMAP( n_components=5).fit_transform(X_train)
# vemos como queda el primer elemento
print(X_train_5D[0])
plt.imshow(np.resize(X_train[0],(28,28)));








class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        
        # Encoder
        
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(8, 16, 
                               kernel_size=3, 
                               stride=2, 
                               padding=1, 
                               output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(16, 1, 
                               kernel_size=3, 
                               stride=2, 
                               padding=1, 
                               output_padding=1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

    def trainModel(self, train_loader):
        criterion = nn.MSELoss()  # Mean Squared Error Loss
        lr = 3e-3
        optimizer = optim.Adam(self.parameters(), lr=lr)
        num_epochs = 2000
        device = torch.device("mps")
        self.to(device)
        losses = []
        for epoch in range(num_epochs):
            self.train()
            running_loss = 0.0
            for batch_idx, data in enumerate(train_loader):
                # Forward pass
                
                data = data.to(device)
                output = self.forward(data)
                loss = criterion(output, data)
                
                # Backward pass and optimization
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                
                running_loss += loss.item()
                losses.append(loss.item)
            if epoch%5 == 0: 
                torch.save(self.state_dict(), 'modelParams')
                if epoch%3 == 0:
                    self.plotCurrentAbility()
                
            #print(f">>epoch [{epoch}], Loss: {running_loss/len(train_loader):.4f}")


                
            print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")
            
        plt.plot(losses)


    def plotCurrentAbility(self):
        self.eval
        self.to('cpu')
        
        plt.subplot(3,2,1)
        plt.imshow(np.reshape(autoencoder(toNNformat(X_train))[0][0].detach().numpy(),(28,28)))
        plt.subplot(3,2,2)
        plt.imshow(np.reshape(X_train[0],(28,28)))
        plt.subplot(3,2,3)
        plt.imshow(np.reshape(autoencoder(toNNformat(X_train))[0][1].detach().numpy(),(28,28)))
        plt.subplot(3,2,4)
        plt.imshow(np.reshape(X_train[1],(28,28)))
        plt.subplot(3,2,5)
        plt.imshow(np.reshape(autoencoder(toNNformat(X_train))[0][2].detach().numpy(),(28,28)))
        plt.subplot(3,2,6)
        plt.imshow(np.reshape(X_train[2],(28,28)))

        self.train()
        self.to('mps')
        

def toNNformat(x):
    xformated = tensor(x).float() / 255.0
    return xformated.unsqueeze(0)


autoencoder = Autoencoder()





X_train_tensor = toNNformat(X_train)
train_loader = DataLoader(X_train_tensor, batch_size=128, shuffle=True)


autoencoder.load_state_dict(torch.load('modelParams'))
autoencoder.trainModel(train_loader)





autoencoder.load_state_dict(torch.load('modelParams'))


autoencoder.eval
autoencoder.to('cpu')

plt.subplot(3,2,1)
plt.imshow(np.reshape(autoencoder(toNNformat(X_train))[0][0].detach().numpy(),(28,28)))
plt.subplot(3,2,2)
plt.imshow(np.reshape(X_train[0],(28,28)))
plt.subplot(3,2,3)
plt.imshow(np.reshape(autoencoder(toNNformat(X_train))[0][1].detach().numpy(),(28,28)))
plt.subplot(3,2,4)
plt.imshow(np.reshape(X_train[1],(28,28)))
plt.subplot(3,2,5)
plt.imshow(np.reshape(autoencoder(toNNformat(X_train))[0][2].detach().numpy(),(28,28)))
plt.subplot(3,2,6)
plt.imshow(np.reshape(X_train[2],(28,28)))











class AgglomerativeClusteringWrapper(AgglomerativeClustering):
    def predict(self,X):
      return self.labels_.astype(int)

class DBSCANWrapper(DBSCAN):
    def predict(self,X):
      return self.labels_.astype(int)

class HDBSCANWrapper(HDBSCAN):
    def predict(self,X):
      return self.labels_.astype(int)

class OPTICSWrapper(OPTICS):
    def predict(self,X):
        return self.labels_.astype(int)

class SpectralClusteringWrapper(SpectralClustering):
    def predict(self,X):
        return self.labels_.astype(int)

clustering_methods = {
    'KMeans': KMeans(),
    'DBSCAN': DBSCANWrapper(),
    'HDBSCAN': HDBSCANWrapper(),
    'MeanShift': MeanShift(),
    'OPTICS': OPTICSWrapper(),
    'AffinityPropagation': AffinityPropagation()
    #'SpectralClustering': SpectralClusteringWrapper(),
    #'Agglomerative': AgglomerativeClusteringWrapper()
    
}



param_grids = {
    'KMeans': {
        'n_clusters': [9],
        'init': ['k-means++', 'random'],
        #'max_iter': [300, 500]
    },
    'DBSCAN': {
        'eps': [10],
        'min_samples': [5, 50]
    },

    'HDBSCAN': {
        'min_cluster_size': [50, 100, 500], 
        'max_cluster_size': [4500, 5500, 6500], 
    },

    'MeanShift': {
        'bandwidth': [3.7,4.5]
    },

    'OPTICS': {
        'min_samples': [50, 100]
    },

    'AffinityPropagation': {
        'damping': [0.5]
    }
    #'SpectralClustering': {
    #    'n_clusters': [9]
    #},
    
    #,
    #'AgglomerativeClustering': {
    #    'n_clusters': [ 9]
    #    #'linkage': ['ward']#, 'complete', 'average']
    #}

}

# Function to evaluate each model and its hyperparameters
def try_clustering(X_train, y_train,  clustering_methods, param_grids):
    results = {}
    best_score = -float('inf')
    for name, model in clustering_methods.items():
        start = time()
        
        grid_search = GridSearchCV(model, param_grids[name], cv=2, scoring='adjusted_mutual_info_score', n_jobs=4)
        grid_search.fit(X_train, y_train)

        
        score = grid_search.best_score_
        if score > best_score:
            model = grid_search.best_estimator_
        params = grid_search.best_params_
        end = time()
        print(f'Modelo: {name}. \nAdjMutualInformation: {score}. \nParametros: {params}. \nTiempo: {round(end-start,2)}s (o {round(end-start,2)/60}min) .\n\n')
        
        results[name] = [params, score, round(end-start,2)]

    results_df = pd.DataFrame.from_dict(results, orient='index', columns=['Mejores parámetros', 'Adjusted Mutual Info Score', 'Seconds'])
    
    # Reset index to make the model name a column
    results_df.reset_index(inplace=True)
    results_df.rename(columns={'index': 'Algoritmo'}, inplace=True)
    
    # Sort the DataFrame by 'Adjusted Mutual Info Score' in descending order
    results_df = results_df.sort_values(by='Adjusted Mutual Info Score', ascending=False)


    return results_df, model


results, model = try_clustering(X_train_5D, y_train, clustering_methods, param_grids)


results









